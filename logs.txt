* 
* ==> Audit <==
* |--------------|---------------------------------|----------|-------|---------|---------------------|---------------------|
|   Command    |              Args               | Profile  | User  | Version |     Start Time      |      End Time       |
|--------------|---------------------------------|----------|-------|---------|---------------------|---------------------|
| image        | load datvi-backend              | minikube | yosua | v1.26.0 | 03 Oct 22 17:33 WIB | 03 Oct 22 17:33 WIB |
|              | datvi-frontend                  |          |       |         |                     |                     |
| image        | ls                              | minikube | yosua | v1.26.0 | 03 Oct 22 17:33 WIB | 03 Oct 22 17:33 WIB |
| update-check |                                 | minikube | yosua | v1.26.1 | 03 Oct 22 17:41 WIB | 03 Oct 22 17:41 WIB |
| update-check |                                 | minikube | yosua | v1.26.1 | 03 Oct 22 17:42 WIB | 03 Oct 22 17:42 WIB |
| kubectl      | -- get pod                      | minikube | yosua | v1.26.0 | 03 Oct 22 17:50 WIB | 03 Oct 22 17:50 WIB |
| kubectl      | -- get svc                      | minikube | yosua | v1.26.0 | 03 Oct 22 17:50 WIB | 03 Oct 22 17:50 WIB |
| kubectl      | -- exec -it                     | minikube | yosua | v1.26.0 | 03 Oct 22 17:50 WIB | 03 Oct 22 17:51 WIB |
|              | datvi-frontend-79d9f45575-prgtw |          |       |         |                     |                     |
|              | /bin/sh                         |          |       |         |                     |                     |
| kubectl      | -- get svc                      | minikube | yosua | v1.26.0 | 03 Oct 22 17:51 WIB | 03 Oct 22 17:51 WIB |
| tunnel       | datvi-frontend                  | minikube | yosua | v1.26.0 | 03 Oct 22 17:51 WIB | 03 Oct 22 17:53 WIB |
| kubectl      | -- get svc                      | minikube | yosua | v1.26.0 | 03 Oct 22 17:52 WIB | 03 Oct 22 17:52 WIB |
| image        | rm datvi-frontend               | minikube | yosua | v1.26.0 | 03 Oct 22 17:54 WIB | 03 Oct 22 17:54 WIB |
| image        | load datvi-frontend             | minikube | yosua | v1.26.0 | 03 Oct 22 17:54 WIB | 03 Oct 22 17:54 WIB |
| kubectl      | -- get pod                      | minikube | yosua | v1.26.0 | 03 Oct 22 18:01 WIB | 03 Oct 22 18:01 WIB |
| kubectl      | -- tunnel datvi-frontend        | minikube | yosua | v1.26.0 | 03 Oct 22 18:01 WIB |                     |
| tunnel       | datvi-frontend                  | minikube | yosua | v1.26.0 | 03 Oct 22 18:01 WIB | 03 Oct 22 18:12 WIB |
| kubectl      | -- get svc                      | minikube | yosua | v1.26.0 | 03 Oct 22 18:02 WIB | 03 Oct 22 18:02 WIB |
| kubectl      | -- get svc                      | minikube | yosua | v1.26.0 | 03 Oct 22 18:02 WIB | 03 Oct 22 18:02 WIB |
| kubectl      | -- get svc                      | minikube | yosua | v1.26.0 | 03 Oct 22 18:12 WIB | 03 Oct 22 18:12 WIB |
| kubectl      | -- get svc                      | minikube | yosua | v1.26.0 | 03 Oct 22 18:12 WIB | 03 Oct 22 18:12 WIB |
| kubectl      | -- get hpa                      | minikube | yosua | v1.26.0 | 03 Oct 22 18:48 WIB | 03 Oct 22 18:48 WIB |
| kubectl      | -- get all                      | minikube | yosua | v1.26.0 | 03 Oct 22 19:52 WIB | 03 Oct 22 19:52 WIB |
| kubectl      | -- get all                      | minikube | yosua | v1.26.0 | 03 Oct 22 19:53 WIB | 03 Oct 22 19:53 WIB |
| kubectl      | -- get all                      | minikube | yosua | v1.26.0 | 03 Oct 22 19:53 WIB | 03 Oct 22 19:53 WIB |
| kubectl      | -- get all                      | minikube | yosua | v1.26.0 | 03 Oct 22 19:53 WIB | 03 Oct 22 19:53 WIB |
| kubectl      | -- get all                      | minikube | yosua | v1.26.0 | 03 Oct 22 19:59 WIB | 03 Oct 22 19:59 WIB |
| image        | load datvi-backend              | minikube | yosua | v1.26.0 | 03 Oct 22 19:59 WIB | 03 Oct 22 20:00 WIB |
|              | datvi-frontend                  |          |       |         |                     |                     |
| image        | load datvi-backend              | minikube | yosua | v1.26.0 | 03 Oct 22 20:00 WIB | 03 Oct 22 20:00 WIB |
|              | datvi-frontend                  |          |       |         |                     |                     |
| tunnel       | datvi-frontend                  | minikube | yosua | v1.26.0 | 03 Oct 22 20:00 WIB | 03 Oct 22 20:01 WIB |
| kubectl      | -- get svc                      | minikube | yosua | v1.26.0 | 03 Oct 22 20:01 WIB | 03 Oct 22 20:01 WIB |
| tunnel       | service                         | minikube | yosua | v1.26.0 | 03 Oct 22 20:02 WIB |                     |
| kubectl      | -- describe svc datvi frontend  | minikube | yosua | v1.26.0 | 03 Oct 22 20:03 WIB |                     |
| kubectl      | -- describe svc datvi-frontend  | minikube | yosua | v1.26.0 | 03 Oct 22 20:03 WIB | 03 Oct 22 20:03 WIB |
| kubectl      | -- get svc --namespace          | minikube | yosua | v1.26.0 | 03 Oct 22 20:04 WIB | 03 Oct 22 20:04 WIB |
|              | default datvi-frontend          |          |       |         |                     |                     |
|              | --template {{ range (index      |          |       |         |                     |                     |
|              | .status.loadBalancer.ingress    |          |       |         |                     |                     |
|              | 0) }}{{.}}{{ end }}             |          |       |         |                     |                     |
| kubectl      | -- get all                      | minikube | yosua | v1.26.0 | 03 Oct 22 20:09 WIB | 03 Oct 22 20:09 WIB |
| image        | ls                              | minikube | yosua | v1.26.0 | 03 Oct 22 20:14 WIB | 03 Oct 22 20:14 WIB |
| image        | load datvi-backend              | minikube | yosua | v1.26.0 | 03 Oct 22 20:14 WIB | 03 Oct 22 20:14 WIB |
|              | datvi-frontend                  |          |       |         |                     |                     |
| tunnel       | datvi-frontend                  | minikube | yosua | v1.26.0 | 03 Oct 22 20:14 WIB |                     |
| image        | rm datvi-frontend               | minikube | yosua | v1.26.0 | 03 Oct 22 20:16 WIB | 03 Oct 22 20:16 WIB |
|              | datvi-backend                   |          |       |         |                     |                     |
| kubectl      | -- get all                      | minikube | yosua | v1.26.0 | 03 Oct 22 20:16 WIB | 03 Oct 22 20:16 WIB |
| kubectl      | -- get all                      | minikube | yosua | v1.26.0 | 03 Oct 22 20:17 WIB | 03 Oct 22 20:17 WIB |
| image        | ls                              | minikube | yosua | v1.26.0 | 03 Oct 22 20:17 WIB | 03 Oct 22 20:17 WIB |
| image        | load datvi-backend              | minikube | yosua | v1.26.0 | 03 Oct 22 21:32 WIB | 03 Oct 22 21:32 WIB |
|              | datvi-frontend                  |          |       |         |                     |                     |
| tunnel       | datvi-frontend                  | minikube | yosua | v1.26.0 | 03 Oct 22 21:32 WIB |                     |
| kubectl      | -- get all                      | minikube | yosua | v1.26.0 | 03 Oct 22 21:34 WIB | 03 Oct 22 21:34 WIB |
| image        | load datvi-backend              | minikube | yosua | v1.26.0 | 03 Oct 22 21:37 WIB | 03 Oct 22 21:38 WIB |
|              | datvi-frontend                  |          |       |         |                     |                     |
| kubectl      | -- get svc --namespace          | minikube | yosua | v1.26.0 | 03 Oct 22 21:39 WIB | 03 Oct 22 21:39 WIB |
|              | default datvi-frontend          |          |       |         |                     |                     |
|              | --template {{ range (index      |          |       |         |                     |                     |
|              | .status.loadBalancer.ingress    |          |       |         |                     |                     |
|              | 0) }}{{.}}{{ end }}             |          |       |         |                     |                     |
| kubectl      | -- get svc                      | minikube | yosua | v1.26.0 | 03 Oct 22 21:45 WIB | 03 Oct 22 21:45 WIB |
| kubectl      | -- get all                      | minikube | yosua | v1.26.0 | 03 Oct 22 21:47 WIB | 03 Oct 22 21:47 WIB |
| image        | ls                              | minikube | yosua | v1.26.0 | 03 Oct 22 21:47 WIB | 03 Oct 22 21:47 WIB |
| image        | load datvi-backend              | minikube | yosua | v1.26.0 | 03 Oct 22 21:47 WIB | 03 Oct 22 21:48 WIB |
|              | datvi-frontend                  |          |       |         |                     |                     |
| kubectl      | -- get svc                      | minikube | yosua | v1.26.0 | 03 Oct 22 21:49 WIB | 03 Oct 22 21:49 WIB |
| image        | ls                              | minikube | yosua | v1.26.0 | 03 Oct 22 21:49 WIB | 03 Oct 22 21:49 WIB |
| image        | load datvi-backend              | minikube | yosua | v1.26.0 | 03 Oct 22 21:50 WIB | 03 Oct 22 21:50 WIB |
|              | datvi-frontend                  |          |       |         |                     |                     |
| tunnel       | datvi-frontend                  | minikube | yosua | v1.26.0 | 03 Oct 22 21:51 WIB |                     |
| kubectl      | -- get svc                      | minikube | yosua | v1.26.0 | 03 Oct 22 21:51 WIB | 03 Oct 22 21:51 WIB |
| image        | load datvi-backend              | minikube | yosua | v1.26.0 | 03 Oct 22 21:52 WIB |                     |
|              | datvi-frontend                  |          |       |         |                     |                     |
| image        | ls                              | minikube | yosua | v1.26.0 | 03 Oct 22 21:52 WIB | 03 Oct 22 21:52 WIB |
| kubectl      | -- get all                      | minikube | yosua | v1.26.0 | 03 Oct 22 21:55 WIB | 03 Oct 22 21:55 WIB |
| delete       |                                 | minikube | yosua | v1.26.0 | 03 Oct 22 21:57 WIB | 03 Oct 22 21:57 WIB |
| start        |                                 | minikube | yosua | v1.26.0 | 03 Oct 22 21:57 WIB | 03 Oct 22 21:57 WIB |
|--------------|---------------------------------|----------|-------|---------|---------------------|---------------------|

* 
* ==> Last Start <==
* Log file created at: 2022/10/03 21:57:15
Running on machine: atlas
Binary: Built with gc go1.18.3 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I1003 21:57:15.051675  398101 out.go:296] Setting OutFile to fd 1 ...
I1003 21:57:15.051789  398101 out.go:348] isatty.IsTerminal(1) = true
I1003 21:57:15.051792  398101 out.go:309] Setting ErrFile to fd 2...
I1003 21:57:15.051796  398101 out.go:348] isatty.IsTerminal(2) = true
I1003 21:57:15.052127  398101 root.go:329] Updating PATH: /home/yosua/.minikube/bin
I1003 21:57:15.052360  398101 out.go:303] Setting JSON to false
I1003 21:57:15.074353  398101 start.go:115] hostinfo: {"hostname":"atlas","uptime":45016,"bootTime":1664764019,"procs":702,"os":"linux","platform":"debian","platformFamily":"debian","platformVersion":"bullseye/sid","kernelVersion":"5.17.5-76051705-generic","kernelArch":"x86_64","virtualizationSystem":"kvm","virtualizationRole":"host","hostId":"789788ba-5721-5f88-39cd-c6b361223c39"}
I1003 21:57:15.074429  398101 start.go:125] virtualization: kvm host
I1003 21:57:15.076171  398101 out.go:177] 😄  minikube v1.26.0 on Debian bullseye/sid
I1003 21:57:15.078849  398101 driver.go:360] Setting default libvirt URI to qemu:///system
I1003 21:57:15.078894  398101 global.go:111] Querying for installed drivers using PATH=/home/yosua/.minikube/bin:/home/yosua/.nvm/versions/node/v16.17.1/bin:/home/yosua/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/usr/local/go/bin:/home/yosua/projects/golang:/home/yosua/projects/golang/bin
I1003 21:57:15.079060  398101 notify.go:193] Checking for updates...
I1003 21:57:15.080605  398101 global.go:119] kvm2 default: true priority: 8, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "virsh": executable file not found in $PATH Reason: Fix:Install libvirt Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/kvm2/ Version:}
I1003 21:57:15.144147  398101 global.go:119] none default: false priority: 4, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1003 21:57:15.144331  398101 global.go:119] podman default: true priority: 7, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "podman": executable file not found in $PATH Reason: Fix:Install Podman Doc:https://minikube.sigs.k8s.io/docs/drivers/podman/ Version:}
I1003 21:57:15.144423  398101 global.go:119] qemu2 default: true priority: 3, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "qemu-system-x86_64": executable file not found in $PATH Reason: Fix:Install qemu-system Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/qemu2/ Version:}
I1003 21:57:15.144435  398101 global.go:119] ssh default: false priority: 4, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1003 21:57:15.227712  398101 virtualbox.go:136] virtual box version: 6.1.36_Popr152435
I1003 21:57:15.227735  398101 global.go:119] virtualbox default: true priority: 6, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:6.1.36_Popr152435
}
I1003 21:57:15.227857  398101 global.go:119] vmware default: true priority: 7, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "docker-machine-driver-vmware": executable file not found in $PATH Reason: Fix:Install docker-machine-driver-vmware Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/vmware/ Version:}
I1003 21:57:15.253443  398101 docker.go:137] docker version: linux-20.10.18
I1003 21:57:15.253550  398101 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1003 21:57:15.564549  398101 info.go:265] docker info: {ID:XSCB:BIVN:JU2N:PGJO:UFN7:UFJ5:HGXD:UV5I:NSHT:EPP7:XVH7:MYXG Containers:2 ContainersRunning:2 ContainersPaused:0 ContainersStopped:0 Images:34 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:true KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:65 OomKillDisable:true NGoroutines:181 SystemTime:2022-10-03 21:57:15.272515682 +0700 WIB LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:2 KernelVersion:5.17.5-76051705-generic OperatingSystem:Pop!_OS 20.04 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:8109748224 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:atlas Labels:[] ExperimentalBuild:false ServerVersion:20.10.18 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID:qb5ukyr4ee9f06tguurqduamf NodeAddr:192.168.0.99 LocalNodeState:active ControlAvailable:true Error: RemoteManagers:[map[Addr:192.168.0.99:2377 NodeID:qb5ukyr4ee9f06tguurqduamf]]} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:9cd3357b7fd7218e4aec3eae239db1f68a5a6ec6 Expected:9cd3357b7fd7218e4aec3eae239db1f68a5a6ec6} RuncCommit:{ID:v1.1.4-0-g5fd4c4d Expected:v1.1.4-0-g5fd4c4d} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=apparmor name=seccomp,profile=default] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Experimental:true Name:app Path:/usr/libexec/docker/cli-plugins/docker-app SchemaVersion:0.1.0 ShortDescription:Docker App Vendor:Docker Inc. Version:v0.9.1-beta3] map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.9.1-docker] map[Name:scan Path:/usr/libexec/docker/cli-plugins/docker-scan SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.17.0]] Warnings:<nil>}}
I1003 21:57:15.564711  398101 docker.go:254] overlay module found
I1003 21:57:15.564723  398101 global.go:119] docker default: true priority: 9, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1003 21:57:15.564741  398101 driver.go:295] not recommending "ssh" due to default: false
I1003 21:57:15.564747  398101 driver.go:295] not recommending "none" due to default: false
I1003 21:57:15.564761  398101 driver.go:330] Picked: docker
I1003 21:57:15.564767  398101 driver.go:331] Alternatives: [virtualbox ssh none]
I1003 21:57:15.564772  398101 driver.go:332] Rejects: [kvm2 podman qemu2 vmware]
I1003 21:57:15.566066  398101 out.go:177] ✨  Automatically selected the docker driver. Other choices: virtualbox, ssh, none
I1003 21:57:15.567184  398101 start.go:284] selected driver: docker
I1003 21:57:15.567188  398101 start.go:805] validating driver "docker" against <nil>
I1003 21:57:15.567214  398101 start.go:816] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1003 21:57:15.567327  398101 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1003 21:57:15.643976  398101 info.go:265] docker info: {ID:XSCB:BIVN:JU2N:PGJO:UFN7:UFJ5:HGXD:UV5I:NSHT:EPP7:XVH7:MYXG Containers:2 ContainersRunning:2 ContainersPaused:0 ContainersStopped:0 Images:34 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:true KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:65 OomKillDisable:true NGoroutines:181 SystemTime:2022-10-03 21:57:15.586843786 +0700 WIB LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:2 KernelVersion:5.17.5-76051705-generic OperatingSystem:Pop!_OS 20.04 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:8109748224 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:atlas Labels:[] ExperimentalBuild:false ServerVersion:20.10.18 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID:qb5ukyr4ee9f06tguurqduamf NodeAddr:192.168.0.99 LocalNodeState:active ControlAvailable:true Error: RemoteManagers:[map[Addr:192.168.0.99:2377 NodeID:qb5ukyr4ee9f06tguurqduamf]]} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:9cd3357b7fd7218e4aec3eae239db1f68a5a6ec6 Expected:9cd3357b7fd7218e4aec3eae239db1f68a5a6ec6} RuncCommit:{ID:v1.1.4-0-g5fd4c4d Expected:v1.1.4-0-g5fd4c4d} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=apparmor name=seccomp,profile=default] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Experimental:true Name:app Path:/usr/libexec/docker/cli-plugins/docker-app SchemaVersion:0.1.0 ShortDescription:Docker App Vendor:Docker Inc. Version:v0.9.1-beta3] map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.9.1-docker] map[Name:scan Path:/usr/libexec/docker/cli-plugins/docker-scan SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.17.0]] Warnings:<nil>}}
I1003 21:57:15.644071  398101 start_flags.go:296] no existing cluster config was found, will generate one from the flags 
I1003 21:57:15.661701  398101 start_flags.go:377] Using suggested 2200MB memory alloc based on sys=7734MB, container=7734MB
I1003 21:57:15.661790  398101 start_flags.go:835] Wait components to verify : map[apiserver:true system_pods:true]
I1003 21:57:15.662823  398101 out.go:177] 📌  Using Docker driver with root privileges
I1003 21:57:15.663760  398101 cni.go:95] Creating CNI manager for ""
I1003 21:57:15.663770  398101 cni.go:169] CNI unnecessary in this configuration, recommending no CNI
I1003 21:57:15.663776  398101 start_flags.go:310] config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.32@sha256:9190bd2393eae887316c97a74370b7d5dad8f0b2ef91ac2662bc36f7ef8e0b95 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.24.1 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/yosua:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath:}
I1003 21:57:15.664964  398101 out.go:177] 👍  Starting control plane node minikube in cluster minikube
I1003 21:57:15.665877  398101 cache.go:120] Beginning downloading kic base image for docker with docker
I1003 21:57:15.666819  398101 out.go:177] 🚜  Pulling base image ...
I1003 21:57:15.668442  398101 preload.go:132] Checking if preload exists for k8s version v1.24.1 and runtime docker
I1003 21:57:15.668499  398101 preload.go:148] Found local preload: /home/yosua/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.24.1-docker-overlay2-amd64.tar.lz4
I1003 21:57:15.668504  398101 cache.go:57] Caching tarball of preloaded images
I1003 21:57:15.668562  398101 image.go:75] Checking for gcr.io/k8s-minikube/kicbase:v0.0.32@sha256:9190bd2393eae887316c97a74370b7d5dad8f0b2ef91ac2662bc36f7ef8e0b95 in local docker daemon
I1003 21:57:15.668701  398101 preload.go:174] Found /home/yosua/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.24.1-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I1003 21:57:15.668708  398101 cache.go:60] Finished verifying existence of preloaded tar for  v1.24.1 on docker
I1003 21:57:15.669121  398101 profile.go:148] Saving config to /home/yosua/.minikube/profiles/minikube/config.json ...
I1003 21:57:15.669144  398101 lock.go:35] WriteFile acquiring /home/yosua/.minikube/profiles/minikube/config.json: {Name:mk2eb55991bb8d23771f9456f9124a43bafe3114 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1003 21:57:15.699687  398101 image.go:79] Found gcr.io/k8s-minikube/kicbase:v0.0.32@sha256:9190bd2393eae887316c97a74370b7d5dad8f0b2ef91ac2662bc36f7ef8e0b95 in local docker daemon, skipping pull
I1003 21:57:15.699698  398101 cache.go:142] gcr.io/k8s-minikube/kicbase:v0.0.32@sha256:9190bd2393eae887316c97a74370b7d5dad8f0b2ef91ac2662bc36f7ef8e0b95 exists in daemon, skipping load
I1003 21:57:15.699710  398101 cache.go:208] Successfully downloaded all kic artifacts
I1003 21:57:15.699736  398101 start.go:352] acquiring machines lock for minikube: {Name:mkc8f2bfeace922a7cada2c67b483ff15f76b64b Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1003 21:57:15.699822  398101 start.go:356] acquired machines lock for "minikube" in 73.897µs
I1003 21:57:15.699834  398101 start.go:91] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.32@sha256:9190bd2393eae887316c97a74370b7d5dad8f0b2ef91ac2662bc36f7ef8e0b95 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.24.1 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.24.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/yosua:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath:} &{Name: IP: Port:8443 KubernetesVersion:v1.24.1 ContainerRuntime:docker ControlPlane:true Worker:true}
I1003 21:57:15.699884  398101 start.go:131] createHost starting for "" (driver="docker")
I1003 21:57:15.701165  398101 out.go:204] 🔥  Creating docker container (CPUs=2, Memory=2200MB) ...
I1003 21:57:15.701645  398101 start.go:165] libmachine.API.Create for "minikube" (driver="docker")
I1003 21:57:15.701662  398101 client.go:168] LocalClient.Create starting
I1003 21:57:15.702233  398101 main.go:134] libmachine: Reading certificate data from /home/yosua/.minikube/certs/ca.pem
I1003 21:57:15.702482  398101 main.go:134] libmachine: Decoding PEM data...
I1003 21:57:15.702498  398101 main.go:134] libmachine: Parsing certificate...
I1003 21:57:15.702948  398101 main.go:134] libmachine: Reading certificate data from /home/yosua/.minikube/certs/cert.pem
I1003 21:57:15.703215  398101 main.go:134] libmachine: Decoding PEM data...
I1003 21:57:15.703236  398101 main.go:134] libmachine: Parsing certificate...
I1003 21:57:15.703659  398101 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
W1003 21:57:15.725554  398101 cli_runner.go:211] docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}" returned with exit code 1
I1003 21:57:15.725626  398101 network_create.go:272] running [docker network inspect minikube] to gather additional debugging logs...
I1003 21:57:15.725644  398101 cli_runner.go:164] Run: docker network inspect minikube
W1003 21:57:15.745911  398101 cli_runner.go:211] docker network inspect minikube returned with exit code 1
I1003 21:57:15.745927  398101 network_create.go:275] error running [docker network inspect minikube]: docker network inspect minikube: exit status 1
stdout:
[]

stderr:
Error: No such network: minikube
I1003 21:57:15.745937  398101 network_create.go:277] output of [docker network inspect minikube]: -- stdout --
[]

-- /stdout --
** stderr ** 
Error: No such network: minikube

** /stderr **
I1003 21:57:15.745980  398101 cli_runner.go:164] Run: docker network inspect bridge --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I1003 21:57:15.765575  398101 network.go:288] reserving subnet 192.168.49.0 for 1m0s: &{mu:{state:0 sema:0} read:{v:{m:map[] amended:true}} dirty:map[192.168.49.0:0xc0007912c0] misses:0}
I1003 21:57:15.765602  398101 network.go:235] using free private subnet 192.168.49.0/24: &{IP:192.168.49.0 Netmask:255.255.255.0 Prefix:24 CIDR:192.168.49.0/24 Gateway:192.168.49.1 ClientMin:192.168.49.2 ClientMax:192.168.49.254 Broadcast:192.168.49.255 Interface:{IfaceName: IfaceIPv4: IfaceMTU:0 IfaceMAC:}}
I1003 21:57:15.765614  398101 network_create.go:115] attempt to create docker network minikube 192.168.49.0/24 with gateway 192.168.49.1 and MTU of 1500 ...
I1003 21:57:15.765657  398101 cli_runner.go:164] Run: docker network create --driver=bridge --subnet=192.168.49.0/24 --gateway=192.168.49.1 -o --ip-masq -o --icc -o com.docker.network.driver.mtu=1500 --label=created_by.minikube.sigs.k8s.io=true --label=name.minikube.sigs.k8s.io=minikube minikube
I1003 21:57:15.841715  398101 network_create.go:99] docker network minikube 192.168.49.0/24 created
I1003 21:57:15.841737  398101 kic.go:106] calculated static IP "192.168.49.2" for the "minikube" container
I1003 21:57:15.841807  398101 cli_runner.go:164] Run: docker ps -a --format {{.Names}}
I1003 21:57:15.865625  398101 cli_runner.go:164] Run: docker volume create minikube --label name.minikube.sigs.k8s.io=minikube --label created_by.minikube.sigs.k8s.io=true
I1003 21:57:15.885598  398101 oci.go:103] Successfully created a docker volume minikube
I1003 21:57:15.885728  398101 cli_runner.go:164] Run: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.32@sha256:9190bd2393eae887316c97a74370b7d5dad8f0b2ef91ac2662bc36f7ef8e0b95 -d /var/lib
I1003 21:57:16.795733  398101 oci.go:107] Successfully prepared a docker volume minikube
I1003 21:57:16.795759  398101 preload.go:132] Checking if preload exists for k8s version v1.24.1 and runtime docker
I1003 21:57:16.795772  398101 kic.go:179] Starting extracting preloaded images to volume ...
I1003 21:57:16.795818  398101 cli_runner.go:164] Run: docker run --rm --entrypoint /usr/bin/tar -v /home/yosua/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.24.1-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.32@sha256:9190bd2393eae887316c97a74370b7d5dad8f0b2ef91ac2662bc36f7ef8e0b95 -I lz4 -xf /preloaded.tar -C /extractDir
I1003 21:57:20.960949  398101 cli_runner.go:217] Completed: docker run --rm --entrypoint /usr/bin/tar -v /home/yosua/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.24.1-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.32@sha256:9190bd2393eae887316c97a74370b7d5dad8f0b2ef91ac2662bc36f7ef8e0b95 -I lz4 -xf /preloaded.tar -C /extractDir: (4.164935894s)
I1003 21:57:20.961037  398101 kic.go:188] duration metric: took 4.165260 seconds to extract preloaded images to volume
W1003 21:57:20.961481  398101 cgroups_linux.go:77] Your kernel does not support swap limit capabilities or the cgroup is not mounted.
I1003 21:57:20.961564  398101 cli_runner.go:164] Run: docker info --format "'{{json .SecurityOptions}}'"
I1003 21:57:21.034704  398101 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=2200mb --cpus=2 -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.32@sha256:9190bd2393eae887316c97a74370b7d5dad8f0b2ef91ac2662bc36f7ef8e0b95
I1003 21:57:21.503051  398101 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Running}}
I1003 21:57:21.524465  398101 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1003 21:57:21.546691  398101 cli_runner.go:164] Run: docker exec minikube stat /var/lib/dpkg/alternatives/iptables
I1003 21:57:21.663838  398101 oci.go:144] the created container "minikube" has a running status.
I1003 21:57:21.663861  398101 kic.go:210] Creating ssh key for kic: /home/yosua/.minikube/machines/minikube/id_rsa...
I1003 21:57:21.784575  398101 kic_runner.go:191] docker (temp): /home/yosua/.minikube/machines/minikube/id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I1003 21:57:21.869051  398101 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1003 21:57:21.900040  398101 kic_runner.go:93] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I1003 21:57:21.900053  398101 kic_runner.go:114] Args: [docker exec --privileged minikube chown docker:docker /home/docker/.ssh/authorized_keys]
I1003 21:57:21.982553  398101 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1003 21:57:22.009055  398101 machine.go:88] provisioning docker machine ...
I1003 21:57:22.009085  398101 ubuntu.go:169] provisioning hostname "minikube"
I1003 21:57:22.009138  398101 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1003 21:57:22.037491  398101 main.go:134] libmachine: Using SSH client type: native
I1003 21:57:22.037651  398101 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x7dae00] 0x7dde60 <nil>  [] 0s} 127.0.0.1 49162 <nil> <nil>}
I1003 21:57:22.037661  398101 main.go:134] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I1003 21:57:22.038316  398101 main.go:134] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:46806->127.0.0.1:49162: read: connection reset by peer
I1003 21:57:25.225649  398101 main.go:134] libmachine: SSH cmd err, output: <nil>: minikube

I1003 21:57:25.225734  398101 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1003 21:57:25.246022  398101 main.go:134] libmachine: Using SSH client type: native
I1003 21:57:25.246147  398101 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x7dae00] 0x7dde60 <nil>  [] 0s} 127.0.0.1 49162 <nil> <nil>}
I1003 21:57:25.246160  398101 main.go:134] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I1003 21:57:25.379405  398101 main.go:134] libmachine: SSH cmd err, output: <nil>: 
I1003 21:57:25.379431  398101 ubuntu.go:175] set auth options {CertDir:/home/yosua/.minikube CaCertPath:/home/yosua/.minikube/certs/ca.pem CaPrivateKeyPath:/home/yosua/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/yosua/.minikube/machines/server.pem ServerKeyPath:/home/yosua/.minikube/machines/server-key.pem ClientKeyPath:/home/yosua/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/yosua/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/yosua/.minikube}
I1003 21:57:25.379468  398101 ubuntu.go:177] setting up certificates
I1003 21:57:25.379487  398101 provision.go:83] configureAuth start
I1003 21:57:25.379566  398101 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1003 21:57:25.401666  398101 provision.go:138] copyHostCerts
I1003 21:57:25.401729  398101 exec_runner.go:144] found /home/yosua/.minikube/cert.pem, removing ...
I1003 21:57:25.401735  398101 exec_runner.go:207] rm: /home/yosua/.minikube/cert.pem
I1003 21:57:25.402130  398101 exec_runner.go:151] cp: /home/yosua/.minikube/certs/cert.pem --> /home/yosua/.minikube/cert.pem (1119 bytes)
I1003 21:57:25.402490  398101 exec_runner.go:144] found /home/yosua/.minikube/key.pem, removing ...
I1003 21:57:25.402493  398101 exec_runner.go:207] rm: /home/yosua/.minikube/key.pem
I1003 21:57:25.402582  398101 exec_runner.go:151] cp: /home/yosua/.minikube/certs/key.pem --> /home/yosua/.minikube/key.pem (1679 bytes)
I1003 21:57:25.403120  398101 exec_runner.go:144] found /home/yosua/.minikube/ca.pem, removing ...
I1003 21:57:25.403123  398101 exec_runner.go:207] rm: /home/yosua/.minikube/ca.pem
I1003 21:57:25.404869  398101 exec_runner.go:151] cp: /home/yosua/.minikube/certs/ca.pem --> /home/yosua/.minikube/ca.pem (1074 bytes)
I1003 21:57:25.404932  398101 provision.go:112] generating server cert: /home/yosua/.minikube/machines/server.pem ca-key=/home/yosua/.minikube/certs/ca.pem private-key=/home/yosua/.minikube/certs/ca-key.pem org=yosua.minikube san=[192.168.49.2 127.0.0.1 localhost 127.0.0.1 minikube minikube]
I1003 21:57:25.564303  398101 provision.go:172] copyRemoteCerts
I1003 21:57:25.564340  398101 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I1003 21:57:25.564364  398101 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1003 21:57:25.584496  398101 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49162 SSHKeyPath:/home/yosua/.minikube/machines/minikube/id_rsa Username:docker}
I1003 21:57:25.677574  398101 ssh_runner.go:362] scp /home/yosua/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1074 bytes)
I1003 21:57:25.709947  398101 ssh_runner.go:362] scp /home/yosua/.minikube/machines/server.pem --> /etc/docker/server.pem (1196 bytes)
I1003 21:57:25.734271  398101 ssh_runner.go:362] scp /home/yosua/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I1003 21:57:25.752396  398101 provision.go:86] duration metric: configureAuth took 372.897382ms
I1003 21:57:25.752410  398101 ubuntu.go:193] setting minikube options for container-runtime
I1003 21:57:25.752599  398101 config.go:178] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.24.1
I1003 21:57:25.752657  398101 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1003 21:57:25.771005  398101 main.go:134] libmachine: Using SSH client type: native
I1003 21:57:25.771122  398101 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x7dae00] 0x7dde60 <nil>  [] 0s} 127.0.0.1 49162 <nil> <nil>}
I1003 21:57:25.771130  398101 main.go:134] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I1003 21:57:25.901840  398101 main.go:134] libmachine: SSH cmd err, output: <nil>: overlay

I1003 21:57:25.901861  398101 ubuntu.go:71] root file system type: overlay
I1003 21:57:25.902130  398101 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I1003 21:57:25.902203  398101 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1003 21:57:25.922062  398101 main.go:134] libmachine: Using SSH client type: native
I1003 21:57:25.922306  398101 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x7dae00] 0x7dde60 <nil>  [] 0s} 127.0.0.1 49162 <nil> <nil>}
I1003 21:57:25.922383  398101 main.go:134] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I1003 21:57:26.062799  398101 main.go:134] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I1003 21:57:26.062889  398101 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1003 21:57:26.100770  398101 main.go:134] libmachine: Using SSH client type: native
I1003 21:57:26.100894  398101 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x7dae00] 0x7dde60 <nil>  [] 0s} 127.0.0.1 49162 <nil> <nil>}
I1003 21:57:26.100908  398101 main.go:134] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I1003 21:57:26.791153  398101 main.go:134] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2022-06-06 23:01:03.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2022-10-03 14:57:26.057182757 +0000
@@ -1,30 +1,32 @@
 [Unit]
 Description=Docker Application Container Engine
 Documentation=https://docs.docker.com
-After=network-online.target docker.socket firewalld.service containerd.service
+BindsTo=containerd.service
+After=network-online.target firewalld.service containerd.service
 Wants=network-online.target
-Requires=docker.socket containerd.service
+Requires=docker.socket
+StartLimitBurst=3
+StartLimitIntervalSec=60
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutSec=0
-RestartSec=2
-Restart=always
-
-# Note that StartLimit* options were moved from "Service" to "Unit" in systemd 229.
-# Both the old, and new location are accepted by systemd 229 and up, so using the old location
-# to make them work for either version of systemd.
-StartLimitBurst=3
+Restart=on-failure
 
-# Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.
-# Both the old, and new name are accepted by systemd 230 and up, so using the old name to make
-# this option work for either version of systemd.
-StartLimitInterval=60s
+
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
 
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
@@ -32,16 +34,16 @@
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
 
 # kill only the docker process, not all processes in the cgroup
 KillMode=process
-OOMScoreAdjust=-500
 
 [Install]
 WantedBy=multi-user.target
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I1003 21:57:26.791179  398101 machine.go:91] provisioned docker machine in 4.7821109s
I1003 21:57:26.791188  398101 client.go:171] LocalClient.Create took 11.089521255s
I1003 21:57:26.791203  398101 start.go:173] duration metric: libmachine.API.Create for "minikube" took 11.089555819s
I1003 21:57:26.791211  398101 start.go:306] post-start starting for "minikube" (driver="docker")
I1003 21:57:26.791217  398101 start.go:316] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I1003 21:57:26.791272  398101 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I1003 21:57:26.791315  398101 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1003 21:57:26.814025  398101 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49162 SSHKeyPath:/home/yosua/.minikube/machines/minikube/id_rsa Username:docker}
I1003 21:57:26.923715  398101 ssh_runner.go:195] Run: cat /etc/os-release
I1003 21:57:26.928952  398101 main.go:134] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I1003 21:57:26.928980  398101 main.go:134] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I1003 21:57:26.928998  398101 main.go:134] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I1003 21:57:26.929008  398101 info.go:137] Remote host: Ubuntu 20.04.4 LTS
I1003 21:57:26.929025  398101 filesync.go:126] Scanning /home/yosua/.minikube/addons for local assets ...
I1003 21:57:26.929442  398101 filesync.go:126] Scanning /home/yosua/.minikube/files for local assets ...
I1003 21:57:26.929714  398101 start.go:309] post-start completed in 138.495571ms
I1003 21:57:26.930192  398101 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1003 21:57:26.948954  398101 profile.go:148] Saving config to /home/yosua/.minikube/profiles/minikube/config.json ...
I1003 21:57:26.949145  398101 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I1003 21:57:26.949174  398101 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1003 21:57:26.966861  398101 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49162 SSHKeyPath:/home/yosua/.minikube/machines/minikube/id_rsa Username:docker}
I1003 21:57:27.061178  398101 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I1003 21:57:27.068401  398101 start.go:134] duration metric: createHost completed in 11.368498348s
I1003 21:57:27.068431  398101 start.go:81] releasing machines lock for "minikube", held for 11.368598053s
I1003 21:57:27.068627  398101 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1003 21:57:27.090315  398101 ssh_runner.go:195] Run: systemctl --version
I1003 21:57:27.090316  398101 ssh_runner.go:195] Run: curl -sS -m 2 https://k8s.gcr.io/
I1003 21:57:27.090346  398101 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1003 21:57:27.090356  398101 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1003 21:57:27.112132  398101 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49162 SSHKeyPath:/home/yosua/.minikube/machines/minikube/id_rsa Username:docker}
I1003 21:57:27.112397  398101 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49162 SSHKeyPath:/home/yosua/.minikube/machines/minikube/id_rsa Username:docker}
I1003 21:57:27.436531  398101 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I1003 21:57:27.453112  398101 cruntime.go:273] skipping containerd shutdown because we are bound to it
I1003 21:57:27.453248  398101 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I1003 21:57:27.469355  398101 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
image-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I1003 21:57:27.487841  398101 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I1003 21:57:27.558265  398101 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I1003 21:57:27.633101  398101 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1003 21:57:27.706671  398101 ssh_runner.go:195] Run: sudo systemctl restart docker
I1003 21:57:27.899018  398101 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1003 21:57:27.969962  398101 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1003 21:57:28.039749  398101 ssh_runner.go:195] Run: sudo systemctl start cri-docker.socket
I1003 21:57:28.050045  398101 start.go:447] Will wait 60s for socket path /var/run/cri-dockerd.sock
I1003 21:57:28.060308  398101 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I1003 21:57:28.063568  398101 start.go:468] Will wait 60s for crictl version
I1003 21:57:28.063604  398101 ssh_runner.go:195] Run: sudo crictl version
I1003 21:57:28.326898  398101 start.go:477] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  20.10.17
RuntimeApiVersion:  1.41.0
I1003 21:57:28.326964  398101 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1003 21:57:28.466062  398101 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1003 21:57:28.502456  398101 out.go:204] 🐳  Preparing Kubernetes v1.24.1 on Docker 20.10.17 ...
I1003 21:57:28.502567  398101 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I1003 21:57:28.522558  398101 ssh_runner.go:195] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I1003 21:57:28.527370  398101 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.49.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1003 21:57:28.540245  398101 preload.go:132] Checking if preload exists for k8s version v1.24.1 and runtime docker
I1003 21:57:28.540284  398101 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1003 21:57:28.571641  398101 docker.go:602] Got preloaded images: -- stdout --
k8s.gcr.io/kube-apiserver:v1.24.1
k8s.gcr.io/kube-controller-manager:v1.24.1
k8s.gcr.io/kube-proxy:v1.24.1
k8s.gcr.io/kube-scheduler:v1.24.1
k8s.gcr.io/etcd:3.5.3-0
k8s.gcr.io/pause:3.7
k8s.gcr.io/coredns/coredns:v1.8.6
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1003 21:57:28.571656  398101 docker.go:533] Images already preloaded, skipping extraction
I1003 21:57:28.571736  398101 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1003 21:57:28.603008  398101 docker.go:602] Got preloaded images: -- stdout --
k8s.gcr.io/kube-apiserver:v1.24.1
k8s.gcr.io/kube-controller-manager:v1.24.1
k8s.gcr.io/kube-proxy:v1.24.1
k8s.gcr.io/kube-scheduler:v1.24.1
k8s.gcr.io/etcd:3.5.3-0
k8s.gcr.io/pause:3.7
k8s.gcr.io/coredns/coredns:v1.8.6
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1003 21:57:28.603030  398101 cache_images.go:84] Images are preloaded, skipping loading
I1003 21:57:28.603102  398101 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I1003 21:57:28.878505  398101 cni.go:95] Creating CNI manager for ""
I1003 21:57:28.878514  398101 cni.go:169] CNI unnecessary in this configuration, recommending no CNI
I1003 21:57:28.878535  398101 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I1003 21:57:28.878547  398101 kubeadm.go:158] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.24.1 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NoTaintMaster:true NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[]}
I1003 21:57:28.878646  398101 kubeadm.go:162] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: /var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.24.1
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I1003 21:57:28.878706  398101 kubeadm.go:961] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.24.1/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime=remote --container-runtime-endpoint=/var/run/cri-dockerd.sock --hostname-override=minikube --image-service-endpoint=/var/run/cri-dockerd.sock --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2 --runtime-request-timeout=15m

[Install]
 config:
{KubernetesVersion:v1.24.1 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I1003 21:57:28.878759  398101 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.24.1
I1003 21:57:28.888318  398101 binaries.go:44] Found k8s binaries, skipping transfer
I1003 21:57:28.888365  398101 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I1003 21:57:28.894651  398101 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (470 bytes)
I1003 21:57:28.910128  398101 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I1003 21:57:28.923361  398101 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2031 bytes)
I1003 21:57:28.938491  398101 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I1003 21:57:28.940924  398101 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1003 21:57:28.949907  398101 certs.go:54] Setting up /home/yosua/.minikube/profiles/minikube for IP: 192.168.49.2
I1003 21:57:28.950000  398101 certs.go:182] skipping minikubeCA CA generation: /home/yosua/.minikube/ca.key
I1003 21:57:28.950274  398101 certs.go:182] skipping proxyClientCA CA generation: /home/yosua/.minikube/proxy-client-ca.key
I1003 21:57:28.950466  398101 certs.go:302] generating minikube-user signed cert: /home/yosua/.minikube/profiles/minikube/client.key
I1003 21:57:28.950475  398101 crypto.go:68] Generating cert /home/yosua/.minikube/profiles/minikube/client.crt with IP's: []
I1003 21:57:29.111204  398101 crypto.go:156] Writing cert to /home/yosua/.minikube/profiles/minikube/client.crt ...
I1003 21:57:29.111217  398101 lock.go:35] WriteFile acquiring /home/yosua/.minikube/profiles/minikube/client.crt: {Name:mk1b25cfd30d9d8382a269aed5ea3334b1ab1c39 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1003 21:57:29.111466  398101 crypto.go:164] Writing key to /home/yosua/.minikube/profiles/minikube/client.key ...
I1003 21:57:29.111472  398101 lock.go:35] WriteFile acquiring /home/yosua/.minikube/profiles/minikube/client.key: {Name:mk67fc3a694df1543e645fcd424f9a6294a29f3d Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1003 21:57:29.111538  398101 certs.go:302] generating minikube signed cert: /home/yosua/.minikube/profiles/minikube/apiserver.key.dd3b5fb2
I1003 21:57:29.111544  398101 crypto.go:68] Generating cert /home/yosua/.minikube/profiles/minikube/apiserver.crt.dd3b5fb2 with IP's: [192.168.49.2 10.96.0.1 127.0.0.1 10.0.0.1]
I1003 21:57:29.297947  398101 crypto.go:156] Writing cert to /home/yosua/.minikube/profiles/minikube/apiserver.crt.dd3b5fb2 ...
I1003 21:57:29.297959  398101 lock.go:35] WriteFile acquiring /home/yosua/.minikube/profiles/minikube/apiserver.crt.dd3b5fb2: {Name:mk2780c0123248da711632a8b28c84847e382aa8 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1003 21:57:29.298108  398101 crypto.go:164] Writing key to /home/yosua/.minikube/profiles/minikube/apiserver.key.dd3b5fb2 ...
I1003 21:57:29.298113  398101 lock.go:35] WriteFile acquiring /home/yosua/.minikube/profiles/minikube/apiserver.key.dd3b5fb2: {Name:mk4afaa2ecbc34e1dd1f53589eca88186a9acddf Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1003 21:57:29.298183  398101 certs.go:320] copying /home/yosua/.minikube/profiles/minikube/apiserver.crt.dd3b5fb2 -> /home/yosua/.minikube/profiles/minikube/apiserver.crt
I1003 21:57:29.298224  398101 certs.go:324] copying /home/yosua/.minikube/profiles/minikube/apiserver.key.dd3b5fb2 -> /home/yosua/.minikube/profiles/minikube/apiserver.key
I1003 21:57:29.302648  398101 certs.go:302] generating aggregator signed cert: /home/yosua/.minikube/profiles/minikube/proxy-client.key
I1003 21:57:29.302669  398101 crypto.go:68] Generating cert /home/yosua/.minikube/profiles/minikube/proxy-client.crt with IP's: []
I1003 21:57:29.634891  398101 crypto.go:156] Writing cert to /home/yosua/.minikube/profiles/minikube/proxy-client.crt ...
I1003 21:57:29.634902  398101 lock.go:35] WriteFile acquiring /home/yosua/.minikube/profiles/minikube/proxy-client.crt: {Name:mkf100b2cda98a26e3885455557d2aa88b03440d Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1003 21:57:29.635092  398101 crypto.go:164] Writing key to /home/yosua/.minikube/profiles/minikube/proxy-client.key ...
I1003 21:57:29.635098  398101 lock.go:35] WriteFile acquiring /home/yosua/.minikube/profiles/minikube/proxy-client.key: {Name:mk81f3a3c769ffaf803b7201bac3502c448fbe67 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1003 21:57:29.639951  398101 certs.go:388] found cert: /home/yosua/.minikube/certs/home/yosua/.minikube/certs/ca-key.pem (1679 bytes)
I1003 21:57:29.639972  398101 certs.go:388] found cert: /home/yosua/.minikube/certs/home/yosua/.minikube/certs/ca.pem (1074 bytes)
I1003 21:57:29.639986  398101 certs.go:388] found cert: /home/yosua/.minikube/certs/home/yosua/.minikube/certs/cert.pem (1119 bytes)
I1003 21:57:29.640015  398101 certs.go:388] found cert: /home/yosua/.minikube/certs/home/yosua/.minikube/certs/key.pem (1679 bytes)
I1003 21:57:29.640804  398101 ssh_runner.go:362] scp /home/yosua/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I1003 21:57:29.656866  398101 ssh_runner.go:362] scp /home/yosua/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I1003 21:57:29.674243  398101 ssh_runner.go:362] scp /home/yosua/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I1003 21:57:29.694557  398101 ssh_runner.go:362] scp /home/yosua/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I1003 21:57:29.716738  398101 ssh_runner.go:362] scp /home/yosua/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I1003 21:57:29.736327  398101 ssh_runner.go:362] scp /home/yosua/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I1003 21:57:29.753331  398101 ssh_runner.go:362] scp /home/yosua/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I1003 21:57:29.770542  398101 ssh_runner.go:362] scp /home/yosua/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I1003 21:57:29.787673  398101 ssh_runner.go:362] scp /home/yosua/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I1003 21:57:29.806641  398101 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I1003 21:57:29.819460  398101 ssh_runner.go:195] Run: openssl version
I1003 21:57:29.827579  398101 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I1003 21:57:29.835404  398101 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I1003 21:57:29.838289  398101 certs.go:431] hashing: -rw-r--r-- 1 root root 1111 Jul 28 08:14 /usr/share/ca-certificates/minikubeCA.pem
I1003 21:57:29.838351  398101 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I1003 21:57:29.843017  398101 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I1003 21:57:29.849839  398101 kubeadm.go:395] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.32@sha256:9190bd2393eae887316c97a74370b7d5dad8f0b2ef91ac2662bc36f7ef8e0b95 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.24.1 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.24.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/yosua:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath:}
I1003 21:57:29.849925  398101 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1003 21:57:29.878158  398101 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I1003 21:57:29.883985  398101 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I1003 21:57:29.890407  398101 kubeadm.go:221] ignoring SystemVerification for kubeadm because of docker driver
I1003 21:57:29.890450  398101 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I1003 21:57:29.897279  398101 kubeadm.go:152] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I1003 21:57:29.897303  398101 ssh_runner.go:286] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.24.1:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables"
I1003 21:57:30.438773  398101 out.go:204]     ▪ Generating certificates and keys ...
I1003 21:57:33.113868  398101 out.go:204]     ▪ Booting up control plane ...
I1003 21:57:54.661440  398101 out.go:204]     ▪ Configuring RBAC rules ...
I1003 21:57:55.077893  398101 cni.go:95] Creating CNI manager for ""
I1003 21:57:55.077915  398101 cni.go:169] CNI unnecessary in this configuration, recommending no CNI
I1003 21:57:55.078332  398101 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.24.1/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig
I1003 21:57:55.078529  398101 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.24.1/kubectl label nodes minikube.k8s.io/version=v1.26.0 minikube.k8s.io/commit=f4b412861bb746be73053c9f6d2895f12cf78565 minikube.k8s.io/name=minikube minikube.k8s.io/updated_at=2022_10_03T21_57_55_0700 minikube.k8s.io/primary=true --all --overwrite --kubeconfig=/var/lib/minikube/kubeconfig
I1003 21:57:55.078874  398101 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I1003 21:57:55.553843  398101 kubeadm.go:1045] duration metric: took 475.5533ms to wait for elevateKubeSystemPrivileges.
I1003 21:57:55.645809  398101 ops.go:34] apiserver oom_adj: -16
I1003 21:57:55.645825  398101 kubeadm.go:397] StartCluster complete in 25.795990989s
I1003 21:57:55.646378  398101 settings.go:142] acquiring lock: {Name:mke697f4f546f3df4d3edae7f196973f0f2490e0 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1003 21:57:55.646615  398101 settings.go:150] Updating kubeconfig:  /home/yosua/.kube/config
I1003 21:57:55.651804  398101 lock.go:35] WriteFile acquiring /home/yosua/.kube/config: {Name:mk3b2bb9fadfea81dbbc06867c5e82c8d7640f35 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1003 21:57:56.194800  398101 kapi.go:244] deployment "coredns" in namespace "kube-system" and context "minikube" rescaled to 1
I1003 21:57:56.195083  398101 start.go:208] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.24.1 ContainerRuntime:docker ControlPlane:true Worker:true}
I1003 21:57:56.197819  398101 out.go:177] 🔎  Verifying Kubernetes components...
I1003 21:57:56.195373  398101 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.24.1/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I1003 21:57:56.195191  398101 addons.go:412] enableAddons start: toEnable=map[], additional=[]
I1003 21:57:56.198763  398101 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I1003 21:57:56.196663  398101 config.go:178] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.24.1
I1003 21:57:56.199234  398101 addons.go:65] Setting storage-provisioner=true in profile "minikube"
I1003 21:57:56.199234  398101 addons.go:65] Setting default-storageclass=true in profile "minikube"
I1003 21:57:56.199248  398101 addons.go:153] Setting addon storage-provisioner=true in "minikube"
I1003 21:57:56.199250  398101 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
W1003 21:57:56.199252  398101 addons.go:162] addon storage-provisioner should already be in state true
I1003 21:57:56.200057  398101 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1003 21:57:56.200467  398101 host.go:66] Checking if "minikube" exists ...
I1003 21:57:56.200944  398101 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1003 21:57:56.245075  398101 out.go:177]     ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I1003 21:57:56.246087  398101 addons.go:345] installing /etc/kubernetes/addons/storage-provisioner.yaml
I1003 21:57:56.246098  398101 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I1003 21:57:56.246147  398101 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1003 21:57:56.248192  398101 addons.go:153] Setting addon default-storageclass=true in "minikube"
W1003 21:57:56.248206  398101 addons.go:162] addon default-storageclass should already be in state true
I1003 21:57:56.248226  398101 host.go:66] Checking if "minikube" exists ...
I1003 21:57:56.248515  398101 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1003 21:57:56.280904  398101 addons.go:345] installing /etc/kubernetes/addons/storageclass.yaml
I1003 21:57:56.280919  398101 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I1003 21:57:56.280979  398101 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1003 21:57:56.281370  398101 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.24.1/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.49.1 host.minikube.internal\n           fallthrough\n        }' | sudo /var/lib/minikube/binaries/v1.24.1/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -"
I1003 21:57:56.282037  398101 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49162 SSHKeyPath:/home/yosua/.minikube/machines/minikube/id_rsa Username:docker}
I1003 21:57:56.284971  398101 api_server.go:51] waiting for apiserver process to appear ...
I1003 21:57:56.285058  398101 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1003 21:57:56.310126  398101 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49162 SSHKeyPath:/home/yosua/.minikube/machines/minikube/id_rsa Username:docker}
I1003 21:57:56.436317  398101 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.24.1/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I1003 21:57:56.458456  398101 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.24.1/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I1003 21:57:57.744080  398101 ssh_runner.go:235] Completed: sudo pgrep -xnf kube-apiserver.*minikube.*: (1.459004756s)
I1003 21:57:57.744113  398101 ssh_runner.go:235] Completed: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.24.1/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.49.1 host.minikube.internal\n           fallthrough\n        }' | sudo /var/lib/minikube/binaries/v1.24.1/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -": (1.462700462s)
I1003 21:57:57.744148  398101 start.go:806] {"host.minikube.internal": 192.168.49.1} host record injected into CoreDNS
I1003 21:57:57.744099  398101 api_server.go:71] duration metric: took 1.548998732s to wait for apiserver process to appear ...
I1003 21:57:57.744360  398101 api_server.go:87] waiting for apiserver healthz status ...
I1003 21:57:57.744705  398101 api_server.go:240] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1003 21:57:57.758701  398101 api_server.go:266] https://192.168.49.2:8443/healthz returned 200:
ok
I1003 21:57:57.760157  398101 api_server.go:140] control plane version: v1.24.1
I1003 21:57:57.760179  398101 api_server.go:130] duration metric: took 15.8072ms to wait for apiserver health ...
I1003 21:57:57.760910  398101 system_pods.go:43] waiting for kube-system pods to appear ...
I1003 21:57:57.771568  398101 system_pods.go:59] 4 kube-system pods found
I1003 21:57:57.771588  398101 system_pods.go:61] "etcd-minikube" [04b9c26d-2cac-4e1d-861b-c3193b187937] Pending
I1003 21:57:57.771592  398101 system_pods.go:61] "kube-apiserver-minikube" [8fedfb1a-7da5-4e36-93ea-0b845eb235a3] Pending
I1003 21:57:57.771596  398101 system_pods.go:61] "kube-controller-manager-minikube" [f317976c-ab21-4b27-9d71-0208551ad016] Pending
I1003 21:57:57.771609  398101 system_pods.go:61] "kube-scheduler-minikube" [1f183769-d368-40b4-b4dc-164a48b449a7] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I1003 21:57:57.771614  398101 system_pods.go:74] duration metric: took 10.695304ms to wait for pod list to return data ...
I1003 21:57:57.771622  398101 kubeadm.go:572] duration metric: took 1.576521552s to wait for : map[apiserver:true system_pods:true] ...
I1003 21:57:57.771637  398101 node_conditions.go:102] verifying NodePressure condition ...
I1003 21:57:57.776250  398101 node_conditions.go:122] node storage ephemeral capacity is 102626228Ki
I1003 21:57:57.776435  398101 node_conditions.go:123] node cpu capacity is 8
I1003 21:57:57.777113  398101 node_conditions.go:105] duration metric: took 5.468814ms to run NodePressure ...
I1003 21:57:57.777146  398101 start.go:213] waiting for startup goroutines ...
I1003 21:57:57.810537  398101 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.24.1/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (1.352056867s)
I1003 21:57:57.810652  398101 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.24.1/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (1.374306978s)
I1003 21:57:57.813328  398101 out.go:177] 🌟  Enabled addons: default-storageclass
I1003 21:57:57.814207  398101 addons.go:414] enableAddons completed in 1.619019526s
I1003 21:57:57.816113  398101 out.go:177] 💡  kubectl not found. If you need it, try: 'minikube kubectl -- get pods -A'
I1003 21:57:57.817219  398101 out.go:177] 🏄  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default

* 
* ==> Docker <==
* -- Logs begin at Mon 2022-10-03 14:57:22 UTC, end at Mon 2022-10-03 14:59:14 UTC. --
Oct 03 14:57:26 minikube systemd[1]: docker.service: Current command vanished from the unit file, execution of the command list won't be resumed.
Oct 03 14:57:26 minikube systemd[1]: Stopping Docker Application Container Engine...
Oct 03 14:57:26 minikube dockerd[260]: time="2022-10-03T14:57:26.584536658Z" level=info msg="Processing signal 'terminated'"
Oct 03 14:57:26 minikube dockerd[260]: time="2022-10-03T14:57:26.601700256Z" level=info msg="stopping event stream following graceful shutdown" error="<nil>" module=libcontainerd namespace=moby
Oct 03 14:57:26 minikube dockerd[260]: time="2022-10-03T14:57:26.602642873Z" level=info msg="Daemon shutdown complete"
Oct 03 14:57:26 minikube systemd[1]: docker.service: Succeeded.
Oct 03 14:57:26 minikube systemd[1]: Stopped Docker Application Container Engine.
Oct 03 14:57:26 minikube systemd[1]: Starting Docker Application Container Engine...
Oct 03 14:57:26 minikube dockerd[500]: time="2022-10-03T14:57:26.647936019Z" level=info msg="Starting up"
Oct 03 14:57:26 minikube dockerd[500]: time="2022-10-03T14:57:26.649637616Z" level=info msg="parsed scheme: \"unix\"" module=grpc
Oct 03 14:57:26 minikube dockerd[500]: time="2022-10-03T14:57:26.649686103Z" level=info msg="scheme \"unix\" not registered, fallback to default scheme" module=grpc
Oct 03 14:57:26 minikube dockerd[500]: time="2022-10-03T14:57:26.649705688Z" level=info msg="ccResolverWrapper: sending update to cc: {[{unix:///run/containerd/containerd.sock  <nil> 0 <nil>}] <nil> <nil>}" module=grpc
Oct 03 14:57:26 minikube dockerd[500]: time="2022-10-03T14:57:26.649718518Z" level=info msg="ClientConn switching balancer to \"pick_first\"" module=grpc
Oct 03 14:57:26 minikube dockerd[500]: time="2022-10-03T14:57:26.650565542Z" level=info msg="parsed scheme: \"unix\"" module=grpc
Oct 03 14:57:26 minikube dockerd[500]: time="2022-10-03T14:57:26.650582868Z" level=info msg="scheme \"unix\" not registered, fallback to default scheme" module=grpc
Oct 03 14:57:26 minikube dockerd[500]: time="2022-10-03T14:57:26.650596538Z" level=info msg="ccResolverWrapper: sending update to cc: {[{unix:///run/containerd/containerd.sock  <nil> 0 <nil>}] <nil> <nil>}" module=grpc
Oct 03 14:57:26 minikube dockerd[500]: time="2022-10-03T14:57:26.650603598Z" level=info msg="ClientConn switching balancer to \"pick_first\"" module=grpc
Oct 03 14:57:26 minikube dockerd[500]: time="2022-10-03T14:57:26.655494261Z" level=info msg="[graphdriver] using prior storage driver: overlay2"
Oct 03 14:57:26 minikube dockerd[500]: time="2022-10-03T14:57:26.659405256Z" level=warning msg="Your kernel does not support CPU realtime scheduler"
Oct 03 14:57:26 minikube dockerd[500]: time="2022-10-03T14:57:26.659423960Z" level=warning msg="Your kernel does not support cgroup blkio weight"
Oct 03 14:57:26 minikube dockerd[500]: time="2022-10-03T14:57:26.659429392Z" level=warning msg="Your kernel does not support cgroup blkio weight_device"
Oct 03 14:57:26 minikube dockerd[500]: time="2022-10-03T14:57:26.659545448Z" level=info msg="Loading containers: start."
Oct 03 14:57:26 minikube dockerd[500]: time="2022-10-03T14:57:26.736661909Z" level=info msg="Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used to set a preferred IP address"
Oct 03 14:57:26 minikube dockerd[500]: time="2022-10-03T14:57:26.766331766Z" level=info msg="Loading containers: done."
Oct 03 14:57:26 minikube dockerd[500]: time="2022-10-03T14:57:26.777794090Z" level=info msg="Docker daemon" commit=a89b842 graphdriver(s)=overlay2 version=20.10.17
Oct 03 14:57:26 minikube dockerd[500]: time="2022-10-03T14:57:26.777858215Z" level=info msg="Daemon has completed initialization"
Oct 03 14:57:26 minikube systemd[1]: Started Docker Application Container Engine.
Oct 03 14:57:26 minikube dockerd[500]: time="2022-10-03T14:57:26.796163089Z" level=info msg="API listen on [::]:2376"
Oct 03 14:57:26 minikube dockerd[500]: time="2022-10-03T14:57:26.800022216Z" level=info msg="API listen on /var/run/docker.sock"
Oct 03 14:57:27 minikube systemd[1]: Stopping Docker Application Container Engine...
Oct 03 14:57:27 minikube dockerd[500]: time="2022-10-03T14:57:27.714580423Z" level=info msg="Processing signal 'terminated'"
Oct 03 14:57:27 minikube dockerd[500]: time="2022-10-03T14:57:27.715743604Z" level=info msg="stopping event stream following graceful shutdown" error="<nil>" module=libcontainerd namespace=moby
Oct 03 14:57:27 minikube dockerd[500]: time="2022-10-03T14:57:27.716607712Z" level=info msg="Daemon shutdown complete"
Oct 03 14:57:27 minikube systemd[1]: docker.service: Succeeded.
Oct 03 14:57:27 minikube systemd[1]: Stopped Docker Application Container Engine.
Oct 03 14:57:27 minikube systemd[1]: Starting Docker Application Container Engine...
Oct 03 14:57:27 minikube dockerd[706]: time="2022-10-03T14:57:27.756950374Z" level=info msg="Starting up"
Oct 03 14:57:27 minikube dockerd[706]: time="2022-10-03T14:57:27.758564740Z" level=info msg="parsed scheme: \"unix\"" module=grpc
Oct 03 14:57:27 minikube dockerd[706]: time="2022-10-03T14:57:27.758587225Z" level=info msg="scheme \"unix\" not registered, fallback to default scheme" module=grpc
Oct 03 14:57:27 minikube dockerd[706]: time="2022-10-03T14:57:27.758605458Z" level=info msg="ccResolverWrapper: sending update to cc: {[{unix:///run/containerd/containerd.sock  <nil> 0 <nil>}] <nil> <nil>}" module=grpc
Oct 03 14:57:27 minikube dockerd[706]: time="2022-10-03T14:57:27.758618255Z" level=info msg="ClientConn switching balancer to \"pick_first\"" module=grpc
Oct 03 14:57:27 minikube dockerd[706]: time="2022-10-03T14:57:27.760889587Z" level=info msg="parsed scheme: \"unix\"" module=grpc
Oct 03 14:57:27 minikube dockerd[706]: time="2022-10-03T14:57:27.760911905Z" level=info msg="scheme \"unix\" not registered, fallback to default scheme" module=grpc
Oct 03 14:57:27 minikube dockerd[706]: time="2022-10-03T14:57:27.760929507Z" level=info msg="ccResolverWrapper: sending update to cc: {[{unix:///run/containerd/containerd.sock  <nil> 0 <nil>}] <nil> <nil>}" module=grpc
Oct 03 14:57:27 minikube dockerd[706]: time="2022-10-03T14:57:27.760941551Z" level=info msg="ClientConn switching balancer to \"pick_first\"" module=grpc
Oct 03 14:57:27 minikube dockerd[706]: time="2022-10-03T14:57:27.765324189Z" level=info msg="[graphdriver] using prior storage driver: overlay2"
Oct 03 14:57:27 minikube dockerd[706]: time="2022-10-03T14:57:27.769516393Z" level=warning msg="Your kernel does not support CPU realtime scheduler"
Oct 03 14:57:27 minikube dockerd[706]: time="2022-10-03T14:57:27.769533130Z" level=warning msg="Your kernel does not support cgroup blkio weight"
Oct 03 14:57:27 minikube dockerd[706]: time="2022-10-03T14:57:27.769537028Z" level=warning msg="Your kernel does not support cgroup blkio weight_device"
Oct 03 14:57:27 minikube dockerd[706]: time="2022-10-03T14:57:27.769640720Z" level=info msg="Loading containers: start."
Oct 03 14:57:27 minikube dockerd[706]: time="2022-10-03T14:57:27.845966854Z" level=info msg="Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used to set a preferred IP address"
Oct 03 14:57:27 minikube dockerd[706]: time="2022-10-03T14:57:27.875728320Z" level=info msg="Loading containers: done."
Oct 03 14:57:27 minikube dockerd[706]: time="2022-10-03T14:57:27.886402814Z" level=info msg="Docker daemon" commit=a89b842 graphdriver(s)=overlay2 version=20.10.17
Oct 03 14:57:27 minikube dockerd[706]: time="2022-10-03T14:57:27.886450483Z" level=info msg="Daemon has completed initialization"
Oct 03 14:57:27 minikube systemd[1]: Started Docker Application Container Engine.
Oct 03 14:57:27 minikube dockerd[706]: time="2022-10-03T14:57:27.903515689Z" level=info msg="API listen on [::]:2376"
Oct 03 14:57:27 minikube dockerd[706]: time="2022-10-03T14:57:27.907427189Z" level=info msg="API listen on /var/run/docker.sock"
Oct 03 14:57:34 minikube dockerd[706]: time="2022-10-03T14:57:34.954542337Z" level=info msg="Attempting next endpoint for pull after error: Head \"https://k8s.gcr.io/v2/pause/manifests/3.6\": read tcp 192.168.49.2:50534->74.125.68.82:443: read: connection reset by peer"
Oct 03 14:57:34 minikube dockerd[706]: time="2022-10-03T14:57:34.957483971Z" level=error msg="Handler for POST /v1.40/images/create returned error: Head \"https://k8s.gcr.io/v2/pause/manifests/3.6\": read tcp 192.168.49.2:50534->74.125.68.82:443: read: connection reset by peer"
Oct 03 14:57:48 minikube dockerd[706]: time="2022-10-03T14:57:48.860527999Z" level=info msg="ignoring event" container=385118303d02f28deba8b8481b62ab935706add46574fc9b10f6c61a05c19a11 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"

* 
* ==> container status <==
* CONTAINER           IMAGE               CREATED              STATE               NAME                      ATTEMPT             POD ID
f9190c79efc50       a4ca41631cc7a       About a minute ago   Running             coredns                   0                   c0d6950716777
d43c3e1add9a8       beb86f5d8e6cd       About a minute ago   Running             kube-proxy                0                   a89d247c2037a
74e6ffa009b9b       6e38f40d628db       About a minute ago   Running             storage-provisioner       0                   867eb451c08c0
b1b8a3c47a40d       b4ea7e648530d       About a minute ago   Running             kube-controller-manager   1                   494227103fd07
fffe285771f61       e9f4b425f9192       About a minute ago   Running             kube-apiserver            0                   9582bd79ac747
68b94ff3b5a8c       18688a72645c5       About a minute ago   Running             kube-scheduler            0                   9098ba74bfdbb
385118303d02f       b4ea7e648530d       About a minute ago   Exited              kube-controller-manager   0                   494227103fd07
270aeb65920a1       aebe758cef4cd       About a minute ago   Running             etcd                      0                   8c3961e83100a

* 
* ==> coredns [f9190c79efc5] <==
* .:53
[INFO] plugin/reload: Running configuration MD5 = cec3c60eb1cc4909fd4579a8d79ea031
CoreDNS-1.8.6
linux/amd64, go1.17.1, 13a9191

* 
* ==> describe nodes <==
* Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=f4b412861bb746be73053c9f6d2895f12cf78565
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2022_10_03T21_57_55_0700
                    minikube.k8s.io/version=v1.26.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Mon, 03 Oct 2022 14:57:53 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Mon, 03 Oct 2022 14:59:06 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Mon, 03 Oct 2022 14:58:05 +0000   Mon, 03 Oct 2022 14:57:53 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Mon, 03 Oct 2022 14:58:05 +0000   Mon, 03 Oct 2022 14:57:53 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Mon, 03 Oct 2022 14:58:05 +0000   Mon, 03 Oct 2022 14:57:53 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Mon, 03 Oct 2022 14:58:05 +0000   Mon, 03 Oct 2022 14:58:05 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                8
  ephemeral-storage:  102626228Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             7919676Ki
  pods:               110
Allocatable:
  cpu:                8
  ephemeral-storage:  102626228Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             7919676Ki
  pods:               110
System Info:
  Machine ID:                 d8902d1345bb469697278da23257a8d2
  System UUID:                f26eb199-c05a-4763-903a-aacecb6a7b42
  Boot ID:                    c62bc31f-b1d4-4689-9084-a4528beacdb8
  Kernel Version:             5.17.5-76051705-generic
  OS Image:                   Ubuntu 20.04.4 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://20.10.17
  Kubelet Version:            v1.24.1
  Kube-Proxy Version:         v1.24.1
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (7 in total)
  Namespace                   Name                                CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                ------------  ----------  ---------------  -------------  ---
  kube-system                 coredns-6d4b75cb6d-f77bp            100m (1%!)(MISSING)     0 (0%!)(MISSING)      70Mi (0%!)(MISSING)        170Mi (2%!)(MISSING)     67s
  kube-system                 etcd-minikube                       100m (1%!)(MISSING)     0 (0%!)(MISSING)      100Mi (1%!)(MISSING)       0 (0%!)(MISSING)         79s
  kube-system                 kube-apiserver-minikube             250m (3%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         79s
  kube-system                 kube-controller-manager-minikube    200m (2%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         79s
  kube-system                 kube-proxy-xl7xj                    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         67s
  kube-system                 kube-scheduler-minikube             100m (1%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         79s
  kube-system                 storage-provisioner                 0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         77s
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (9%!)(MISSING)   0 (0%!)(MISSING)
  memory             170Mi (2%!)(MISSING)  170Mi (2%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
Events:
  Type    Reason                   Age   From             Message
  ----    ------                   ----  ----             -------
  Normal  Starting                 65s   kube-proxy       
  Normal  Starting                 80s   kubelet          Starting kubelet.
  Normal  NodeHasSufficientMemory  79s   kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    79s   kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     79s   kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  79s   kubelet          Updated Node Allocatable limit across pods
  Normal  NodeReady                69s   kubelet          Node minikube status is now: NodeReady
  Normal  RegisteredNode           67s   node-controller  Node minikube event: Registered Node minikube in Controller

* 
* ==> dmesg <==
* 
[  +0.000001] No Arguments are initialized for method [_CPC]

[  +0.000001] ACPI Error: Aborting method \_SB.PR07._CPC due to previous error (AE_NOT_FOUND) (20211217/psparse-529)
[  +0.009568] hpet_acpi_add: no address or irqs in _CRS
[  +0.015113] device-mapper: core: CONFIG_IMA_DISABLE_HTABLE is disabled. Duplicate IMA measurements will not be recorded in the IMA log.
[  +0.000053] platform eisa.0: EISA: Cannot allocate resource for mainboard
[  +0.000001] platform eisa.0: Cannot allocate resource for EISA slot 1
[  +0.000001] platform eisa.0: Cannot allocate resource for EISA slot 2
[  +0.000001] platform eisa.0: Cannot allocate resource for EISA slot 3
[  +0.000001] platform eisa.0: Cannot allocate resource for EISA slot 4
[  +0.000000] platform eisa.0: Cannot allocate resource for EISA slot 5
[  +0.000001] platform eisa.0: Cannot allocate resource for EISA slot 6
[  +0.000001] platform eisa.0: Cannot allocate resource for EISA slot 7
[  +0.000000] platform eisa.0: Cannot allocate resource for EISA slot 8
[Oct 3 02:27] acpi PNP0C14:02: duplicate WMI GUID 05901221-D566-11D1-B2F0-00A0C9062910 (first instance was on PNP0C14:01)
[  +0.002318] acpi PNP0C14:03: duplicate WMI GUID 05901221-D566-11D1-B2F0-00A0C9062910 (first instance was on PNP0C14:01)
[  +0.000030] wmi_bus wmi_bus-PNP0C14:03: WQBJ data block query control method not found
[  +0.000003] wmi_bus wmi_bus-PNP0C14:03: WQBK data block query control method not found
[  +0.026959] ACPI BIOS Error (bug): Could not resolve symbol [\_SB.IPPF._STA.POS1], AE_NOT_FOUND (20211217/psargs-330)

[  +0.000010] No Local Variables are initialized for Method [_STA]

[  +0.000001] No Arguments are initialized for method [_STA]

[  +0.000001] ACPI Error: Aborting method \_SB.IPPF._STA due to previous error (AE_NOT_FOUND) (20211217/psparse-529)
[  +0.026528] ACPI BIOS Error (bug): Could not resolve symbol [\_SB.IPPF._STA.POS1], AE_NOT_FOUND (20211217/psargs-330)

[  +0.000010] No Local Variables are initialized for Method [_STA]

[  +0.000002] No Arguments are initialized for method [_STA]

[  +0.000002] ACPI Error: Aborting method \_SB.IPPF._STA due to previous error (AE_NOT_FOUND) (20211217/psparse-529)
[  +0.297551] ACPI BIOS Error (bug): Could not resolve symbol [\_SB.IPPF._STA.POS1], AE_NOT_FOUND (20211217/psargs-330)

[  +0.000040] No Local Variables are initialized for Method [_STA]

[  +0.000001] No Arguments are initialized for method [_STA]

[  +0.000002] ACPI Error: Aborting method \_SB.IPPF._STA due to previous error (AE_NOT_FOUND) (20211217/psparse-529)
[  +0.433992] system76_acpi: loading out-of-tree module taints kernel.
[  +1.595545] Userspace governor deprecated: use thermal netlink notification instead
[  +0.253296] iwlwifi 0000:00:14.3: Direct firmware load for iwlwifi-Qu-c0-jf-b0-69.ucode failed with error -2
[  +0.000035] iwlwifi 0000:00:14.3: Direct firmware load for iwlwifi-Qu-c0-jf-b0-68.ucode failed with error -2
[  +0.000044] iwlwifi 0000:00:14.3: Direct firmware load for iwlwifi-Qu-c0-jf-b0-67.ucode failed with error -2
[  +0.000576] iwlwifi 0000:00:14.3: Direct firmware load for iwlwifi-Qu-c0-jf-b0-66.ucode failed with error -2
[  +0.000374] iwlwifi 0000:00:14.3: Direct firmware load for iwlwifi-Qu-c0-jf-b0-65.ucode failed with error -2
[  +0.000700] iwlwifi 0000:00:14.3: Direct firmware load for iwlwifi-Qu-c0-jf-b0-64.ucode failed with error -2
[  +0.006801] iwlwifi 0000:00:14.3: api flags index 2 larger than supported by driver
[  +0.149296] thermal thermal_zone5: failed to read out thermal zone (-61)
[  +0.526768] FAT-fs (nvme0n1p1): Volume was not properly unmounted. Some data may be corrupt. Please run fsck.
[  +0.932436] kauditd_printk_skb: 27 callbacks suppressed
[  +0.309316] hp_wmi: query 0x4c returned error 0x6
[  +3.946080] VBoxNetFlt: Successfully started.
[  +0.009838] VBoxNetAdp: Successfully started.
[Oct 3 07:38] iwlwifi 0000:00:14.3: Unhandled alg: 0x3f0707
[  +0.000011] iwlwifi 0000:00:14.3: Unhandled alg: 0x3f0707
[Oct 3 11:04] iwlwifi 0000:00:14.3: Unhandled alg: 0x3f0707
[Oct 3 14:10] iwlwifi 0000:00:14.3: Unhandled alg: 0x3f0707
[Oct 3 14:30] iwlwifi 0000:00:14.3: No beacon heard and the session protection is over already...

* 
* ==> etcd [270aeb65920a] <==
* {"level":"info","ts":"2022-10-03T14:57:38.056Z","caller":"etcdmain/etcd.go:73","msg":"Running: ","args":["etcd","--advertise-client-urls=https://192.168.49.2:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--experimental-initial-corrupt-check=true","--initial-advertise-peer-urls=https://192.168.49.2:2380","--initial-cluster=minikube=https://192.168.49.2:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://192.168.49.2:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://192.168.49.2:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"info","ts":"2022-10-03T14:57:38.057Z","caller":"embed/etcd.go:131","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2022-10-03T14:57:38.057Z","caller":"embed/etcd.go:479","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2022-10-03T14:57:38.058Z","caller":"embed/etcd.go:139","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"]}
{"level":"info","ts":"2022-10-03T14:57:38.058Z","caller":"embed/etcd.go:308","msg":"starting an etcd server","etcd-version":"3.5.3","git-sha":"0452feec7","go-version":"go1.16.15","go-os":"linux","go-arch":"amd64","max-cpu-set":8,"max-cpu-available":8,"member-initialized":false,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"minikube=https://192.168.49.2:2380","initial-cluster-state":"new","initial-cluster-token":"etcd-cluster","quota-size-bytes":2147483648,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2022-10-03T14:57:38.065Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"4.203831ms"}
{"level":"info","ts":"2022-10-03T14:57:38.069Z","caller":"etcdserver/raft.go:448","msg":"starting local member","local-member-id":"aec36adc501070cc","cluster-id":"fa54960ea34d58be"}
{"level":"info","ts":"2022-10-03T14:57:38.069Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=()"}
{"level":"info","ts":"2022-10-03T14:57:38.069Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 0"}
{"level":"info","ts":"2022-10-03T14:57:38.069Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [], term: 0, commit: 0, applied: 0, lastindex: 0, lastterm: 0]"}
{"level":"info","ts":"2022-10-03T14:57:38.069Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 1"}
{"level":"info","ts":"2022-10-03T14:57:38.069Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"warn","ts":"2022-10-03T14:57:38.071Z","caller":"auth/store.go:1220","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2022-10-03T14:57:38.073Z","caller":"mvcc/kvstore.go:415","msg":"kvstore restored","current-rev":1}
{"level":"info","ts":"2022-10-03T14:57:38.096Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2022-10-03T14:57:38.097Z","caller":"etcdserver/server.go:851","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.3","cluster-version":"to_be_decided"}
{"level":"info","ts":"2022-10-03T14:57:38.098Z","caller":"etcdserver/server.go:736","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"aec36adc501070cc","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2022-10-03T14:57:38.100Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2022-10-03T14:57:38.100Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","added-peer-id":"aec36adc501070cc","added-peer-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2022-10-03T14:57:38.100Z","caller":"embed/etcd.go:688","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2022-10-03T14:57:38.101Z","caller":"embed/etcd.go:581","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2022-10-03T14:57:38.101Z","caller":"embed/etcd.go:553","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2022-10-03T14:57:38.101Z","caller":"embed/etcd.go:277","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2022-10-03T14:57:38.101Z","caller":"embed/etcd.go:763","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2022-10-03T14:57:38.670Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 1"}
{"level":"info","ts":"2022-10-03T14:57:38.670Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 1"}
{"level":"info","ts":"2022-10-03T14:57:38.670Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 1"}
{"level":"info","ts":"2022-10-03T14:57:38.670Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 2"}
{"level":"info","ts":"2022-10-03T14:57:38.670Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 2"}
{"level":"info","ts":"2022-10-03T14:57:38.670Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 2"}
{"level":"info","ts":"2022-10-03T14:57:38.670Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 2"}
{"level":"info","ts":"2022-10-03T14:57:38.670Z","caller":"etcdserver/server.go:2507","msg":"setting up initial cluster version using v2 API","cluster-version":"3.5"}
{"level":"info","ts":"2022-10-03T14:57:38.671Z","caller":"etcdserver/server.go:2042","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2022-10-03T14:57:38.671Z","caller":"embed/serve.go:98","msg":"ready to serve client requests"}
{"level":"info","ts":"2022-10-03T14:57:38.671Z","caller":"embed/serve.go:98","msg":"ready to serve client requests"}
{"level":"info","ts":"2022-10-03T14:57:38.671Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2022-10-03T14:57:38.671Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2022-10-03T14:57:38.671Z","caller":"membership/cluster.go:584","msg":"set initial cluster version","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","cluster-version":"3.5"}
{"level":"info","ts":"2022-10-03T14:57:38.671Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2022-10-03T14:57:38.671Z","caller":"etcdserver/server.go:2531","msg":"cluster version is updated","cluster-version":"3.5"}
{"level":"info","ts":"2022-10-03T14:57:38.672Z","caller":"embed/serve.go:188","msg":"serving client traffic securely","address":"127.0.0.1:2379"}
{"level":"info","ts":"2022-10-03T14:57:38.672Z","caller":"embed/serve.go:188","msg":"serving client traffic securely","address":"192.168.49.2:2379"}

* 
* ==> kernel <==
*  14:59:14 up 12:32,  0 users,  load average: 2.69, 2.44, 1.82
Linux minikube 5.17.5-76051705-generic #202204271406~1653440576~20.04~6277a18-Ubuntu SMP PREEMPT Thu Ma x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 20.04.4 LTS"

* 
* ==> kube-apiserver [fffe285771f6] <==
* W1003 14:57:50.110611       1 genericapiserver.go:557] Skipping API apps/v1beta1 because it has no resources.
W1003 14:57:50.112758       1 genericapiserver.go:557] Skipping API admissionregistration.k8s.io/v1beta1 because it has no resources.
I1003 14:57:50.116798       1 plugins.go:158] Loaded 12 mutating admission controller(s) successfully in the following order: NamespaceLifecycle,LimitRanger,ServiceAccount,NodeRestriction,TaintNodesByCondition,Priority,DefaultTolerationSeconds,DefaultStorageClass,StorageObjectInUseProtection,RuntimeClass,DefaultIngressClass,MutatingAdmissionWebhook.
I1003 14:57:50.116818       1 plugins.go:161] Loaded 11 validating admission controller(s) successfully in the following order: LimitRanger,ServiceAccount,PodSecurity,Priority,PersistentVolumeClaimResize,RuntimeClass,CertificateApproval,CertificateSigning,CertificateSubjectRestriction,ValidatingAdmissionWebhook,ResourceQuota.
W1003 14:57:50.137817       1 genericapiserver.go:557] Skipping API apiregistration.k8s.io/v1beta1 because it has no resources.
I1003 14:57:51.590605       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I1003 14:57:51.590622       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I1003 14:57:51.590857       1 dynamic_serving_content.go:132] "Starting controller" name="serving-cert::/var/lib/minikube/certs/apiserver.crt::/var/lib/minikube/certs/apiserver.key"
I1003 14:57:51.591049       1 secure_serving.go:210] Serving securely on [::]:8443
I1003 14:57:51.591106       1 dynamic_serving_content.go:132] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I1003 14:57:51.591189       1 available_controller.go:491] Starting AvailableConditionController
I1003 14:57:51.591200       1 cache.go:32] Waiting for caches to sync for AvailableConditionController controller
I1003 14:57:51.591220       1 controller.go:80] Starting OpenAPI V3 AggregationController
I1003 14:57:51.591768       1 cluster_authentication_trust_controller.go:440] Starting cluster_authentication_trust_controller controller
I1003 14:57:51.591782       1 shared_informer.go:255] Waiting for caches to sync for cluster_authentication_trust_controller
I1003 14:57:51.592181       1 apf_controller.go:317] Starting API Priority and Fairness config controller
I1003 14:57:51.594605       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I1003 14:57:51.595517       1 apiservice_controller.go:97] Starting APIServiceRegistrationController
I1003 14:57:51.595662       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I1003 14:57:51.595714       1 autoregister_controller.go:141] Starting autoregister controller
I1003 14:57:51.595724       1 cache.go:32] Waiting for caches to sync for autoregister controller
I1003 14:57:51.595791       1 controller.go:83] Starting OpenAPI AggregationController
I1003 14:57:51.595907       1 customresource_discovery_controller.go:209] Starting DiscoveryController
I1003 14:57:51.596111       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I1003 14:57:51.631969       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I1003 14:57:51.632741       1 controller.go:85] Starting OpenAPI controller
I1003 14:57:51.633719       1 crdregistration_controller.go:111] Starting crd-autoregister controller
I1003 14:57:51.633748       1 shared_informer.go:255] Waiting for caches to sync for crd-autoregister
I1003 14:57:51.637115       1 controller.go:85] Starting OpenAPI V3 controller
I1003 14:57:51.637166       1 naming_controller.go:291] Starting NamingConditionController
I1003 14:57:51.637196       1 establishing_controller.go:76] Starting EstablishingController
I1003 14:57:51.637214       1 nonstructuralschema_controller.go:192] Starting NonStructuralSchemaConditionController
I1003 14:57:51.637240       1 apiapproval_controller.go:186] Starting KubernetesAPIApprovalPolicyConformantConditionController
I1003 14:57:51.637266       1 crd_finalizer.go:266] Starting CRDFinalizer
I1003 14:57:51.653862       1 controller.go:611] quota admission added evaluator for: namespaces
I1003 14:57:51.691540       1 cache.go:39] Caches are synced for AvailableConditionController controller
I1003 14:57:51.692860       1 shared_informer.go:262] Caches are synced for cluster_authentication_trust_controller
I1003 14:57:51.693049       1 apf_controller.go:322] Running API Priority and Fairness config worker
I1003 14:57:51.695943       1 cache.go:39] Caches are synced for autoregister controller
I1003 14:57:51.695966       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I1003 14:57:51.697269       1 shared_informer.go:262] Caches are synced for node_authorizer
I1003 14:57:51.733962       1 shared_informer.go:262] Caches are synced for crd-autoregister
I1003 14:57:52.349583       1 controller.go:132] OpenAPI AggregationController: action for item k8s_internal_local_delegation_chain_0000000000: Nothing (removed from the queue).
I1003 14:57:52.602169       1 storage_scheduling.go:95] created PriorityClass system-node-critical with value 2000001000
I1003 14:57:52.609656       1 storage_scheduling.go:95] created PriorityClass system-cluster-critical with value 2000000000
I1003 14:57:52.609672       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I1003 14:57:52.878133       1 controller.go:611] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I1003 14:57:52.904172       1 controller.go:611] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I1003 14:57:52.962617       1 alloc.go:327] "allocated clusterIPs" service="default/kubernetes" clusterIPs=map[IPv4:10.96.0.1]
W1003 14:57:52.966527       1 lease.go:234] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I1003 14:57:52.967278       1 controller.go:611] quota admission added evaluator for: endpoints
I1003 14:57:52.969893       1 controller.go:611] quota admission added evaluator for: endpointslices.discovery.k8s.io
I1003 14:57:53.758726       1 controller.go:611] quota admission added evaluator for: serviceaccounts
I1003 14:57:54.886300       1 controller.go:611] quota admission added evaluator for: deployments.apps
I1003 14:57:54.893499       1 alloc.go:327] "allocated clusterIPs" service="kube-system/kube-dns" clusterIPs=map[IPv4:10.96.0.10]
I1003 14:57:54.904743       1 controller.go:611] quota admission added evaluator for: daemonsets.apps
I1003 14:57:54.991137       1 controller.go:611] quota admission added evaluator for: leases.coordination.k8s.io
I1003 14:58:07.715163       1 controller.go:611] quota admission added evaluator for: replicasets.apps
I1003 14:58:07.766662       1 controller.go:611] quota admission added evaluator for: controllerrevisions.apps
I1003 14:58:08.618707       1 controller.go:611] quota admission added evaluator for: events.events.k8s.io

* 
* ==> kube-controller-manager [385118303d02] <==
* k8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/util/wait.Until(0xc0001e5a80?, 0x0?, 0xc000180300?)
	vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:90 +0x25
created by k8s.io/kubernetes/vendor/k8s.io/apiserver/pkg/server/dynamiccertificates.(*DynamicServingCertificateController).Run
	vendor/k8s.io/apiserver/pkg/server/dynamiccertificates/tlsconfig.go:250 +0x24a

goroutine 51 [select]:
k8s.io/kubernetes/vendor/k8s.io/apiserver/pkg/server/dynamiccertificates.(*DynamicFileCAContent).watchCAFile(0xc00055e780, 0xc000118480)
	vendor/k8s.io/apiserver/pkg/server/dynamiccertificates/dynamic_cafile_content.go:190 +0x2f6
k8s.io/kubernetes/vendor/k8s.io/apiserver/pkg/server/dynamiccertificates.(*DynamicFileCAContent).Run.func1()
	vendor/k8s.io/apiserver/pkg/server/dynamiccertificates/dynamic_cafile_content.go:165 +0x3c
k8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1(0x1000392e920?)
	vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:155 +0x3e
k8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/util/wait.BackoffUntil(0x0?, {0x4cfc9e0, 0xc0007a4000}, 0x1, 0xc000118480)
	vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:156 +0xb6
k8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/util/wait.JitterUntil(0x0?, 0xdf8475800, 0x0, 0x38?, 0xc000075fd0?)
	vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:133 +0x89
k8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/util/wait.Until(0x4d20960?, 0xc00004ecc0?, 0xc00055e720?)
	vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:90 +0x25
created by k8s.io/kubernetes/vendor/k8s.io/apiserver/pkg/server/dynamiccertificates.(*DynamicFileCAContent).Run
	vendor/k8s.io/apiserver/pkg/server/dynamiccertificates/dynamic_cafile_content.go:164 +0x372

goroutine 138 [select]:
k8s.io/kubernetes/vendor/k8s.io/apiserver/pkg/server/dynamiccertificates.(*DynamicFileCAContent).watchCAFile(0xc00055ea80, 0xc000118480)
	vendor/k8s.io/apiserver/pkg/server/dynamiccertificates/dynamic_cafile_content.go:190 +0x2f6
k8s.io/kubernetes/vendor/k8s.io/apiserver/pkg/server/dynamiccertificates.(*DynamicFileCAContent).Run.func1()
	vendor/k8s.io/apiserver/pkg/server/dynamiccertificates/dynamic_cafile_content.go:165 +0x3c
k8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1(0x392e920?)
	vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:155 +0x3e
k8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/util/wait.BackoffUntil(0x0?, {0x4cfc9e0, 0xc0008b2060}, 0x1, 0xc000118480)
	vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:156 +0xb6
k8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/util/wait.JitterUntil(0x0?, 0xdf8475800, 0x0, 0x38?, 0xc0008c07d0?)
	vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:133 +0x89
k8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/util/wait.Until(0x4d20960?, 0xc00004ecc0?, 0x0?)
	vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:90 +0x25
created by k8s.io/kubernetes/vendor/k8s.io/apiserver/pkg/server/dynamiccertificates.(*DynamicFileCAContent).Run
	vendor/k8s.io/apiserver/pkg/server/dynamiccertificates/dynamic_cafile_content.go:164 +0x372

goroutine 139 [syscall]:
syscall.Syscall6(0xe8, 0xa, 0xc00128fc14, 0x7, 0xffffffffffffffff, 0x0, 0x0)
	/usr/local/go/src/syscall/asm_linux_amd64.s:43 +0x5
k8s.io/kubernetes/vendor/golang.org/x/sys/unix.EpollWait(0x0?, {0xc00128fc14?, 0x0?, 0x0?}, 0x0?)
	vendor/golang.org/x/sys/unix/zsyscall_linux_amd64.go:56 +0x58
k8s.io/kubernetes/vendor/github.com/fsnotify/fsnotify.(*fdPoller).wait(0xc00027a020)
	vendor/github.com/fsnotify/fsnotify/inotify_poller.go:86 +0x7d
k8s.io/kubernetes/vendor/github.com/fsnotify/fsnotify.(*Watcher).readEvents(0xc0003ce050)
	vendor/github.com/fsnotify/fsnotify/inotify.go:192 +0x26e
created by k8s.io/kubernetes/vendor/github.com/fsnotify/fsnotify.NewWatcher
	vendor/github.com/fsnotify/fsnotify/inotify.go:59 +0x1c5

goroutine 52 [syscall]:
syscall.Syscall6(0xe8, 0xd, 0xc0012afc14, 0x7, 0xffffffffffffffff, 0x0, 0x0)
	/usr/local/go/src/syscall/asm_linux_amd64.s:43 +0x5
k8s.io/kubernetes/vendor/golang.org/x/sys/unix.EpollWait(0x0?, {0xc0012afc14?, 0x0?, 0x0?}, 0x0?)
	vendor/golang.org/x/sys/unix/zsyscall_linux_amd64.go:56 +0x58
k8s.io/kubernetes/vendor/github.com/fsnotify/fsnotify.(*fdPoller).wait(0xc000792020)
	vendor/github.com/fsnotify/fsnotify/inotify_poller.go:86 +0x7d
k8s.io/kubernetes/vendor/github.com/fsnotify/fsnotify.(*Watcher).readEvents(0xc0007aa000)
	vendor/github.com/fsnotify/fsnotify/inotify.go:192 +0x26e
created by k8s.io/kubernetes/vendor/github.com/fsnotify/fsnotify.NewWatcher
	vendor/github.com/fsnotify/fsnotify/inotify.go:59 +0x1c5

* 
* ==> kube-controller-manager [b1b8a3c47a40] <==
* I1003 14:58:07.489099       1 controllermanager.go:593] Started "root-ca-cert-publisher"
I1003 14:58:07.489181       1 publisher.go:107] Starting root CA certificate configmap publisher
I1003 14:58:07.489197       1 shared_informer.go:255] Waiting for caches to sync for crt configmap
I1003 14:58:07.493946       1 controllermanager.go:593] Started "endpointslicemirroring"
I1003 14:58:07.494043       1 endpointslicemirroring_controller.go:212] Starting EndpointSliceMirroring controller
I1003 14:58:07.494053       1 shared_informer.go:255] Waiting for caches to sync for endpoint_slice_mirroring
I1003 14:58:07.497006       1 shared_informer.go:255] Waiting for caches to sync for resource quota
W1003 14:58:07.503169       1 actual_state_of_world.go:541] Failed to update statusUpdateNeeded field in actual state of world: Failed to set statusUpdateNeeded to needed true, because nodeName="minikube" does not exist
I1003 14:58:07.505978       1 shared_informer.go:262] Caches are synced for service account
I1003 14:58:07.506645       1 shared_informer.go:262] Caches are synced for bootstrap_signer
I1003 14:58:07.512674       1 shared_informer.go:262] Caches are synced for namespace
I1003 14:58:07.515217       1 shared_informer.go:255] Waiting for caches to sync for garbage collector
I1003 14:58:07.539810       1 shared_informer.go:262] Caches are synced for TTL after finished
I1003 14:58:07.557712       1 shared_informer.go:262] Caches are synced for TTL
I1003 14:58:07.557848       1 shared_informer.go:262] Caches are synced for ClusterRoleAggregator
I1003 14:58:07.580087       1 shared_informer.go:262] Caches are synced for node
I1003 14:58:07.580110       1 range_allocator.go:173] Starting range CIDR allocator
I1003 14:58:07.580116       1 shared_informer.go:255] Waiting for caches to sync for cidrallocator
I1003 14:58:07.580121       1 shared_informer.go:262] Caches are synced for cidrallocator
I1003 14:58:07.583882       1 range_allocator.go:374] Set node minikube PodCIDR to [10.244.0.0/24]
I1003 14:58:07.585003       1 shared_informer.go:262] Caches are synced for expand
I1003 14:58:07.590292       1 shared_informer.go:262] Caches are synced for cronjob
I1003 14:58:07.590296       1 shared_informer.go:262] Caches are synced for crt configmap
I1003 14:58:07.604040       1 shared_informer.go:262] Caches are synced for PV protection
I1003 14:58:07.696789       1 shared_informer.go:262] Caches are synced for certificate-csrsigning-kube-apiserver-client
I1003 14:58:07.696808       1 shared_informer.go:262] Caches are synced for certificate-csrsigning-legacy-unknown
I1003 14:58:07.696812       1 shared_informer.go:262] Caches are synced for certificate-csrsigning-kubelet-serving
I1003 14:58:07.696797       1 shared_informer.go:262] Caches are synced for certificate-csrsigning-kubelet-client
I1003 14:58:07.706552       1 shared_informer.go:262] Caches are synced for deployment
I1003 14:58:07.706558       1 shared_informer.go:262] Caches are synced for PVC protection
I1003 14:58:07.707647       1 shared_informer.go:262] Caches are synced for certificate-csrapproving
I1003 14:58:07.709561       1 shared_informer.go:262] Caches are synced for GC
I1003 14:58:07.716769       1 event.go:294] "Event occurred" object="kube-system/coredns" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set coredns-6d4b75cb6d to 1"
I1003 14:58:07.717584       1 shared_informer.go:262] Caches are synced for HPA
I1003 14:58:07.733725       1 shared_informer.go:262] Caches are synced for attach detach
I1003 14:58:07.738876       1 shared_informer.go:262] Caches are synced for persistent volume
I1003 14:58:07.743024       1 shared_informer.go:262] Caches are synced for ephemeral
I1003 14:58:07.756243       1 shared_informer.go:262] Caches are synced for ReplicationController
I1003 14:58:07.756326       1 shared_informer.go:262] Caches are synced for endpoint
I1003 14:58:07.756346       1 shared_informer.go:262] Caches are synced for endpoint_slice
I1003 14:58:07.756433       1 shared_informer.go:262] Caches are synced for ReplicaSet
I1003 14:58:07.756823       1 shared_informer.go:262] Caches are synced for daemon sets
I1003 14:58:07.758554       1 shared_informer.go:262] Caches are synced for job
I1003 14:58:07.758566       1 shared_informer.go:262] Caches are synced for stateful set
I1003 14:58:07.760468       1 shared_informer.go:262] Caches are synced for taint
I1003 14:58:07.760576       1 node_lifecycle_controller.go:1399] Initializing eviction metric for zone: 
I1003 14:58:07.760641       1 taint_manager.go:187] "Starting NoExecuteTaintManager"
W1003 14:58:07.760649       1 node_lifecycle_controller.go:1014] Missing timestamp for Node minikube. Assuming now as a timestamp.
I1003 14:58:07.760757       1 node_lifecycle_controller.go:1215] Controller detected that zone  is now in state Normal.
I1003 14:58:07.760804       1 event.go:294] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube event: Registered Node minikube in Controller"
I1003 14:58:07.769019       1 event.go:294] "Event occurred" object="kube-system/coredns-6d4b75cb6d" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: coredns-6d4b75cb6d-f77bp"
I1003 14:58:07.775538       1 event.go:294] "Event occurred" object="kube-system/kube-proxy" fieldPath="" kind="DaemonSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: kube-proxy-xl7xj"
I1003 14:58:07.776796       1 shared_informer.go:262] Caches are synced for resource quota
I1003 14:58:07.786074       1 shared_informer.go:262] Caches are synced for disruption
I1003 14:58:07.786092       1 disruption.go:371] Sending events to api server.
I1003 14:58:07.794793       1 shared_informer.go:262] Caches are synced for endpoint_slice_mirroring
I1003 14:58:07.832152       1 shared_informer.go:262] Caches are synced for resource quota
I1003 14:58:08.206182       1 shared_informer.go:262] Caches are synced for garbage collector
I1003 14:58:08.206206       1 garbagecollector.go:158] Garbage collector: all resource monitors have synced. Proceeding to collect garbage
I1003 14:58:08.216401       1 shared_informer.go:262] Caches are synced for garbage collector

* 
* ==> kube-proxy [d43c3e1add9a] <==
* I1003 14:58:08.575576       1 node.go:163] Successfully retrieved node IP: 192.168.49.2
I1003 14:58:08.575865       1 server_others.go:138] "Detected node IP" address="192.168.49.2"
I1003 14:58:08.575949       1 server_others.go:578] "Unknown proxy mode, assuming iptables proxy" proxyMode=""
I1003 14:58:08.613185       1 server_others.go:206] "Using iptables Proxier"
I1003 14:58:08.613230       1 server_others.go:213] "kube-proxy running in dual-stack mode" ipFamily=IPv4
I1003 14:58:08.613238       1 server_others.go:214] "Creating dualStackProxier for iptables"
I1003 14:58:08.613251       1 server_others.go:501] "Detect-local-mode set to ClusterCIDR, but no IPv6 cluster CIDR defined, , defaulting to no-op detect-local for IPv6"
I1003 14:58:08.613674       1 proxier.go:259] "Setting route_localnet=1, use nodePortAddresses to filter loopback addresses for NodePorts to skip it https://issues.k8s.io/90259"
I1003 14:58:08.613931       1 proxier.go:259] "Setting route_localnet=1, use nodePortAddresses to filter loopback addresses for NodePorts to skip it https://issues.k8s.io/90259"
I1003 14:58:08.614362       1 server.go:661] "Version info" version="v1.24.1"
I1003 14:58:08.614395       1 server.go:663] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1003 14:58:08.615605       1 config.go:226] "Starting endpoint slice config controller"
I1003 14:58:08.615611       1 config.go:317] "Starting service config controller"
I1003 14:58:08.616054       1 shared_informer.go:255] Waiting for caches to sync for service config
I1003 14:58:08.616059       1 shared_informer.go:255] Waiting for caches to sync for endpoint slice config
I1003 14:58:08.615640       1 config.go:444] "Starting node config controller"
I1003 14:58:08.616117       1 shared_informer.go:255] Waiting for caches to sync for node config
I1003 14:58:08.716383       1 shared_informer.go:262] Caches are synced for node config
I1003 14:58:08.716383       1 shared_informer.go:262] Caches are synced for endpoint slice config
I1003 14:58:08.716407       1 shared_informer.go:262] Caches are synced for service config

* 
* ==> kube-scheduler [68b94ff3b5a8] <==
* E1003 14:57:41.926409       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: Get "https://192.168.49.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1003 14:57:41.973300       1 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.PodDisruptionBudget: Get "https://192.168.49.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1003 14:57:41.973358       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: Get "https://192.168.49.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1003 14:57:42.008468       1 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.StatefulSet: Get "https://192.168.49.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1003 14:57:42.008517       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: Get "https://192.168.49.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1003 14:57:42.083547       1 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.StorageClass: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1003 14:57:42.083598       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1003 14:57:42.236073       1 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Service: Get "https://192.168.49.2:8443/api/v1/services?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1003 14:57:42.236120       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://192.168.49.2:8443/api/v1/services?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1003 14:57:42.254800       1 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.PersistentVolumeClaim: Get "https://192.168.49.2:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1003 14:57:42.254866       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: Get "https://192.168.49.2:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1003 14:57:42.315976       1 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.ReplicationController: Get "https://192.168.49.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1003 14:57:42.316032       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: Get "https://192.168.49.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1003 14:57:42.499816       1 reflector.go:324] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: failed to list *v1.ConfigMap: Get "https://192.168.49.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%!D(MISSING)extension-apiserver-authentication&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1003 14:57:42.499847       1 reflector.go:138] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get "https://192.168.49.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%!D(MISSING)extension-apiserver-authentication&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1003 14:57:42.719481       1 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Node: Get "https://192.168.49.2:8443/api/v1/nodes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1003 14:57:42.719533       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Node: failed to list *v1.Node: Get "https://192.168.49.2:8443/api/v1/nodes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1003 14:57:42.842854       1 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Pod: Get "https://192.168.49.2:8443/api/v1/pods?fieldSelector=status.phase%3DSucceeded%!C(MISSING)status.phase%3DFailed&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1003 14:57:42.842906       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Pod: failed to list *v1.Pod: Get "https://192.168.49.2:8443/api/v1/pods?fieldSelector=status.phase%3DSucceeded%!C(MISSING)status.phase%3DFailed&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1003 14:57:42.903210       1 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Namespace: Get "https://192.168.49.2:8443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1003 14:57:42.903264       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://192.168.49.2:8443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1003 14:57:43.161406       1 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.CSINode: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1003 14:57:43.161466       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSINode: failed to list *v1.CSINode: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1003 14:57:43.539320       1 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.CSIStorageCapacity: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1003 14:57:43.539381       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1003 14:57:43.714001       1 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.ReplicaSet: Get "https://192.168.49.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1003 14:57:43.714037       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: Get "https://192.168.49.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1003 14:57:44.977728       1 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.CSIDriver: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1003 14:57:44.977760       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1003 14:57:45.300004       1 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.StatefulSet: Get "https://192.168.49.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1003 14:57:45.300036       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: Get "https://192.168.49.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1003 14:57:45.723216       1 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.PersistentVolumeClaim: Get "https://192.168.49.2:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1003 14:57:45.723266       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: Get "https://192.168.49.2:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1003 14:57:46.152611       1 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Node: Get "https://192.168.49.2:8443/api/v1/nodes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1003 14:57:46.152678       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Node: failed to list *v1.Node: Get "https://192.168.49.2:8443/api/v1/nodes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1003 14:57:46.164281       1 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.ReplicationController: Get "https://192.168.49.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1003 14:57:46.164331       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: Get "https://192.168.49.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1003 14:57:46.929255       1 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Pod: Get "https://192.168.49.2:8443/api/v1/pods?fieldSelector=status.phase%3DSucceeded%!C(MISSING)status.phase%3DFailed&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1003 14:57:46.929347       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Pod: failed to list *v1.Pod: Get "https://192.168.49.2:8443/api/v1/pods?fieldSelector=status.phase%3DSucceeded%!C(MISSING)status.phase%3DFailed&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1003 14:57:47.166383       1 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Service: Get "https://192.168.49.2:8443/api/v1/services?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1003 14:57:47.166455       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://192.168.49.2:8443/api/v1/services?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1003 14:57:47.978833       1 reflector.go:324] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: failed to list *v1.ConfigMap: Get "https://192.168.49.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%!D(MISSING)extension-apiserver-authentication&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1003 14:57:47.978881       1 reflector.go:138] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get "https://192.168.49.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%!D(MISSING)extension-apiserver-authentication&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1003 14:57:48.033148       1 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.PersistentVolume: Get "https://192.168.49.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1003 14:57:48.033197       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: Get "https://192.168.49.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1003 14:57:48.129496       1 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.PodDisruptionBudget: Get "https://192.168.49.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1003 14:57:48.129546       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: Get "https://192.168.49.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1003 14:57:48.142289       1 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.ReplicaSet: Get "https://192.168.49.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1003 14:57:48.142354       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: Get "https://192.168.49.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1003 14:57:48.196242       1 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.CSINode: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1003 14:57:48.196292       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSINode: failed to list *v1.CSINode: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1003 14:57:48.316114       1 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.StorageClass: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1003 14:57:48.316144       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1003 14:57:49.101002       1 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.Namespace: Get "https://192.168.49.2:8443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1003 14:57:49.101041       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://192.168.49.2:8443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1003 14:57:51.634318       1 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E1003 14:57:51.634685       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W1003 14:57:51.637826       1 reflector.go:324] vendor/k8s.io/client-go/informers/factory.go:134: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E1003 14:57:51.637858       1 reflector.go:138] vendor/k8s.io/client-go/informers/factory.go:134: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
I1003 14:57:58.104650       1 shared_informer.go:262] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file

* 
* ==> kubelet <==
* -- Logs begin at Mon 2022-10-03 14:57:22 UTC, end at Mon 2022-10-03 14:59:14 UTC. --
Oct 03 14:57:54 minikube kubelet[2176]: I1003 14:57:54.982250    2176 desired_state_of_world_populator.go:145] "Desired state populator starts to run"
Oct 03 14:57:55 minikube kubelet[2176]: I1003 14:57:55.049854    2176 kubelet_network_linux.go:76] "Initialized protocol iptables rules." protocol=IPv4
Oct 03 14:57:55 minikube kubelet[2176]: I1003 14:57:55.139944    2176 kubelet_node_status.go:70] "Attempting to register node" node="minikube"
Oct 03 14:57:55 minikube kubelet[2176]: I1003 14:57:55.146872    2176 kubelet_node_status.go:108] "Node was previously registered" node="minikube"
Oct 03 14:57:55 minikube kubelet[2176]: I1003 14:57:55.146966    2176 kubelet_node_status.go:73] "Successfully registered node" node="minikube"
Oct 03 14:57:55 minikube kubelet[2176]: I1003 14:57:55.178617    2176 kubelet_network_linux.go:76] "Initialized protocol iptables rules." protocol=IPv6
Oct 03 14:57:55 minikube kubelet[2176]: I1003 14:57:55.178637    2176 status_manager.go:161] "Starting to sync pod status with apiserver"
Oct 03 14:57:55 minikube kubelet[2176]: I1003 14:57:55.178655    2176 kubelet.go:1974] "Starting kubelet main sync loop"
Oct 03 14:57:55 minikube kubelet[2176]: E1003 14:57:55.178702    2176 kubelet.go:1998] "Skipping pod synchronization" err="[container runtime status check may not have completed yet, PLEG is not healthy: pleg has yet to be successful]"
Oct 03 14:57:55 minikube kubelet[2176]: E1003 14:57:55.279296    2176 kubelet.go:1998] "Skipping pod synchronization" err="container runtime status check may not have completed yet"
Oct 03 14:57:55 minikube kubelet[2176]: I1003 14:57:55.279341    2176 cpu_manager.go:213] "Starting CPU manager" policy="none"
Oct 03 14:57:55 minikube kubelet[2176]: I1003 14:57:55.279362    2176 cpu_manager.go:214] "Reconciling" reconcilePeriod="10s"
Oct 03 14:57:55 minikube kubelet[2176]: I1003 14:57:55.279399    2176 state_mem.go:36] "Initialized new in-memory state store"
Oct 03 14:57:55 minikube kubelet[2176]: I1003 14:57:55.279992    2176 state_mem.go:88] "Updated default CPUSet" cpuSet=""
Oct 03 14:57:55 minikube kubelet[2176]: I1003 14:57:55.280014    2176 state_mem.go:96] "Updated CPUSet assignments" assignments=map[]
Oct 03 14:57:55 minikube kubelet[2176]: I1003 14:57:55.280022    2176 policy_none.go:49] "None policy: Start"
Oct 03 14:57:55 minikube kubelet[2176]: I1003 14:57:55.283683    2176 memory_manager.go:168] "Starting memorymanager" policy="None"
Oct 03 14:57:55 minikube kubelet[2176]: I1003 14:57:55.283717    2176 state_mem.go:35] "Initializing new in-memory state store"
Oct 03 14:57:55 minikube kubelet[2176]: I1003 14:57:55.283950    2176 state_mem.go:75] "Updated machine memory state"
Oct 03 14:57:55 minikube kubelet[2176]: I1003 14:57:55.338497    2176 manager.go:610] "Failed to read data from checkpoint" checkpoint="kubelet_internal_checkpoint" err="checkpoint is not found"
Oct 03 14:57:55 minikube kubelet[2176]: I1003 14:57:55.339104    2176 plugin_manager.go:114] "Starting Kubelet Plugin Manager"
Oct 03 14:57:55 minikube kubelet[2176]: I1003 14:57:55.532063    2176 topology_manager.go:200] "Topology Admit Handler"
Oct 03 14:57:55 minikube kubelet[2176]: I1003 14:57:55.532364    2176 topology_manager.go:200] "Topology Admit Handler"
Oct 03 14:57:55 minikube kubelet[2176]: I1003 14:57:55.532618    2176 topology_manager.go:200] "Topology Admit Handler"
Oct 03 14:57:55 minikube kubelet[2176]: I1003 14:57:55.532717    2176 topology_manager.go:200] "Topology Admit Handler"
Oct 03 14:57:55 minikube kubelet[2176]: I1003 14:57:55.535285    2176 reconciler.go:270] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/bab0508344d11c6fdb45b1f91c440ff5-kubeconfig\") pod \"kube-scheduler-minikube\" (UID: \"bab0508344d11c6fdb45b1f91c440ff5\") " pod="kube-system/kube-scheduler-minikube"
Oct 03 14:57:55 minikube kubelet[2176]: I1003 14:57:55.636478    2176 reconciler.go:270] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/6580cebb2d04c6c59385cf58e278b0a6-etc-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"6580cebb2d04c6c59385cf58e278b0a6\") " pod="kube-system/kube-apiserver-minikube"
Oct 03 14:57:55 minikube kubelet[2176]: I1003 14:57:55.636519    2176 reconciler.go:270] "operationExecutor.VerifyControllerAttachedVolume started for volume \"flexvolume-dir\" (UniqueName: \"kubernetes.io/host-path/b4f7419eaf4a6f0ee6121d47723a0c8d-flexvolume-dir\") pod \"kube-controller-manager-minikube\" (UID: \"b4f7419eaf4a6f0ee6121d47723a0c8d\") " pod="kube-system/kube-controller-manager-minikube"
Oct 03 14:57:55 minikube kubelet[2176]: I1003 14:57:55.636804    2176 reconciler.go:270] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etcd-data\" (UniqueName: \"kubernetes.io/host-path/906edd533192a4db2396a938662a5271-etcd-data\") pod \"etcd-minikube\" (UID: \"906edd533192a4db2396a938662a5271\") " pod="kube-system/etcd-minikube"
Oct 03 14:57:55 minikube kubelet[2176]: I1003 14:57:55.636871    2176 reconciler.go:270] "operationExecutor.VerifyControllerAttachedVolume started for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/b4f7419eaf4a6f0ee6121d47723a0c8d-ca-certs\") pod \"kube-controller-manager-minikube\" (UID: \"b4f7419eaf4a6f0ee6121d47723a0c8d\") " pod="kube-system/kube-controller-manager-minikube"
Oct 03 14:57:55 minikube kubelet[2176]: I1003 14:57:55.636938    2176 reconciler.go:270] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/b4f7419eaf4a6f0ee6121d47723a0c8d-usr-local-share-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"b4f7419eaf4a6f0ee6121d47723a0c8d\") " pod="kube-system/kube-controller-manager-minikube"
Oct 03 14:57:55 minikube kubelet[2176]: I1003 14:57:55.637013    2176 reconciler.go:270] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etcd-certs\" (UniqueName: \"kubernetes.io/host-path/906edd533192a4db2396a938662a5271-etcd-certs\") pod \"etcd-minikube\" (UID: \"906edd533192a4db2396a938662a5271\") " pod="kube-system/etcd-minikube"
Oct 03 14:57:55 minikube kubelet[2176]: I1003 14:57:55.637050    2176 reconciler.go:270] "operationExecutor.VerifyControllerAttachedVolume started for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/6580cebb2d04c6c59385cf58e278b0a6-ca-certs\") pod \"kube-apiserver-minikube\" (UID: \"6580cebb2d04c6c59385cf58e278b0a6\") " pod="kube-system/kube-apiserver-minikube"
Oct 03 14:57:55 minikube kubelet[2176]: I1003 14:57:55.637073    2176 reconciler.go:270] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/6580cebb2d04c6c59385cf58e278b0a6-usr-local-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"6580cebb2d04c6c59385cf58e278b0a6\") " pod="kube-system/kube-apiserver-minikube"
Oct 03 14:57:55 minikube kubelet[2176]: I1003 14:57:55.637099    2176 reconciler.go:270] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/6580cebb2d04c6c59385cf58e278b0a6-usr-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"6580cebb2d04c6c59385cf58e278b0a6\") " pod="kube-system/kube-apiserver-minikube"
Oct 03 14:57:55 minikube kubelet[2176]: I1003 14:57:55.637151    2176 reconciler.go:270] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/b4f7419eaf4a6f0ee6121d47723a0c8d-etc-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"b4f7419eaf4a6f0ee6121d47723a0c8d\") " pod="kube-system/kube-controller-manager-minikube"
Oct 03 14:57:55 minikube kubelet[2176]: I1003 14:57:55.637188    2176 reconciler.go:270] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/b4f7419eaf4a6f0ee6121d47723a0c8d-usr-share-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"b4f7419eaf4a6f0ee6121d47723a0c8d\") " pod="kube-system/kube-controller-manager-minikube"
Oct 03 14:57:55 minikube kubelet[2176]: I1003 14:57:55.637213    2176 reconciler.go:270] "operationExecutor.VerifyControllerAttachedVolume started for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/6580cebb2d04c6c59385cf58e278b0a6-k8s-certs\") pod \"kube-apiserver-minikube\" (UID: \"6580cebb2d04c6c59385cf58e278b0a6\") " pod="kube-system/kube-apiserver-minikube"
Oct 03 14:57:55 minikube kubelet[2176]: I1003 14:57:55.637246    2176 reconciler.go:270] "operationExecutor.VerifyControllerAttachedVolume started for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/b4f7419eaf4a6f0ee6121d47723a0c8d-k8s-certs\") pod \"kube-controller-manager-minikube\" (UID: \"b4f7419eaf4a6f0ee6121d47723a0c8d\") " pod="kube-system/kube-controller-manager-minikube"
Oct 03 14:57:55 minikube kubelet[2176]: I1003 14:57:55.637264    2176 reconciler.go:270] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/b4f7419eaf4a6f0ee6121d47723a0c8d-kubeconfig\") pod \"kube-controller-manager-minikube\" (UID: \"b4f7419eaf4a6f0ee6121d47723a0c8d\") " pod="kube-system/kube-controller-manager-minikube"
Oct 03 14:57:55 minikube kubelet[2176]: I1003 14:57:55.968825    2176 apiserver.go:52] "Watching apiserver"
Oct 03 14:57:56 minikube kubelet[2176]: I1003 14:57:56.240818    2176 reconciler.go:157] "Reconciler: start to sync state"
Oct 03 14:57:56 minikube kubelet[2176]: E1003 14:57:56.636378    2176 kubelet.go:1690] "Failed creating a mirror pod for" err="pods \"kube-apiserver-minikube\" already exists" pod="kube-system/kube-apiserver-minikube"
Oct 03 14:57:56 minikube kubelet[2176]: E1003 14:57:56.775163    2176 kubelet.go:1690] "Failed creating a mirror pod for" err="pods \"etcd-minikube\" already exists" pod="kube-system/etcd-minikube"
Oct 03 14:57:56 minikube kubelet[2176]: E1003 14:57:56.973241    2176 kubelet.go:1690] "Failed creating a mirror pod for" err="pods \"kube-controller-manager-minikube\" already exists" pod="kube-system/kube-controller-manager-minikube"
Oct 03 14:57:57 minikube kubelet[2176]: E1003 14:57:57.246165    2176 kubelet.go:1690] "Failed creating a mirror pod for" err="pods \"kube-scheduler-minikube\" already exists" pod="kube-system/kube-scheduler-minikube"
Oct 03 14:58:07 minikube kubelet[2176]: I1003 14:58:07.603862    2176 kuberuntime_manager.go:1095] "Updating runtime config through cri with podcidr" CIDR="10.244.0.0/24"
Oct 03 14:58:07 minikube kubelet[2176]: I1003 14:58:07.605705    2176 kubelet_network.go:60] "Updating Pod CIDR" originalPodCIDR="" newPodCIDR="10.244.0.0/24"
Oct 03 14:58:07 minikube kubelet[2176]: I1003 14:58:07.782495    2176 topology_manager.go:200] "Topology Admit Handler"
Oct 03 14:58:07 minikube kubelet[2176]: I1003 14:58:07.784829    2176 topology_manager.go:200] "Topology Admit Handler"
Oct 03 14:58:07 minikube kubelet[2176]: I1003 14:58:07.937740    2176 reconciler.go:270] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/08d6e76e-9f12-40dd-a7ce-acf4dc576e97-lib-modules\") pod \"kube-proxy-xl7xj\" (UID: \"08d6e76e-9f12-40dd-a7ce-acf4dc576e97\") " pod="kube-system/kube-proxy-xl7xj"
Oct 03 14:58:07 minikube kubelet[2176]: I1003 14:58:07.937796    2176 reconciler.go:270] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-r2kqg\" (UniqueName: \"kubernetes.io/projected/08d6e76e-9f12-40dd-a7ce-acf4dc576e97-kube-api-access-r2kqg\") pod \"kube-proxy-xl7xj\" (UID: \"08d6e76e-9f12-40dd-a7ce-acf4dc576e97\") " pod="kube-system/kube-proxy-xl7xj"
Oct 03 14:58:07 minikube kubelet[2176]: I1003 14:58:07.937894    2176 reconciler.go:270] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/08d6e76e-9f12-40dd-a7ce-acf4dc576e97-xtables-lock\") pod \"kube-proxy-xl7xj\" (UID: \"08d6e76e-9f12-40dd-a7ce-acf4dc576e97\") " pod="kube-system/kube-proxy-xl7xj"
Oct 03 14:58:07 minikube kubelet[2176]: I1003 14:58:07.937945    2176 reconciler.go:270] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-proxy\" (UniqueName: \"kubernetes.io/configmap/08d6e76e-9f12-40dd-a7ce-acf4dc576e97-kube-proxy\") pod \"kube-proxy-xl7xj\" (UID: \"08d6e76e-9f12-40dd-a7ce-acf4dc576e97\") " pod="kube-system/kube-proxy-xl7xj"
Oct 03 14:58:07 minikube kubelet[2176]: I1003 14:58:07.937969    2176 reconciler.go:270] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/host-path/117e3870-a468-450f-bc62-6943290a56cb-tmp\") pod \"storage-provisioner\" (UID: \"117e3870-a468-450f-bc62-6943290a56cb\") " pod="kube-system/storage-provisioner"
Oct 03 14:58:07 minikube kubelet[2176]: I1003 14:58:07.938000    2176 reconciler.go:270] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-m4gs8\" (UniqueName: \"kubernetes.io/projected/117e3870-a468-450f-bc62-6943290a56cb-kube-api-access-m4gs8\") pod \"storage-provisioner\" (UID: \"117e3870-a468-450f-bc62-6943290a56cb\") " pod="kube-system/storage-provisioner"
Oct 03 14:58:08 minikube kubelet[2176]: I1003 14:58:08.841149    2176 topology_manager.go:200] "Topology Admit Handler"
Oct 03 14:58:08 minikube kubelet[2176]: I1003 14:58:08.944940    2176 reconciler.go:270] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-2gzfv\" (UniqueName: \"kubernetes.io/projected/b5d23592-0a8d-43f5-88f7-d646dab1bcf0-kube-api-access-2gzfv\") pod \"coredns-6d4b75cb6d-f77bp\" (UID: \"b5d23592-0a8d-43f5-88f7-d646dab1bcf0\") " pod="kube-system/coredns-6d4b75cb6d-f77bp"
Oct 03 14:58:08 minikube kubelet[2176]: I1003 14:58:08.945034    2176 reconciler.go:270] "operationExecutor.VerifyControllerAttachedVolume started for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/b5d23592-0a8d-43f5-88f7-d646dab1bcf0-config-volume\") pod \"coredns-6d4b75cb6d-f77bp\" (UID: \"b5d23592-0a8d-43f5-88f7-d646dab1bcf0\") " pod="kube-system/coredns-6d4b75cb6d-f77bp"
Oct 03 14:58:09 minikube kubelet[2176]: I1003 14:58:09.796486    2176 pod_container_deletor.go:79] "Container not found in pod's containers" containerID="c0d6950716777a37723b6fa620ee21fb0551b204ac9dc644078f6c7b53a4faa6"

* 
* ==> storage-provisioner [74e6ffa009b9] <==
* I1003 14:58:08.517728       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I1003 14:58:12.667128       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I1003 14:58:12.667350       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I1003 14:58:12.675559       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I1003 14:58:12.675724       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_a3ac7a74-0e64-41bb-a2c4-a46c44eb5f68!
I1003 14:58:12.675684       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"104d6ac1-b45b-40cd-800a-3995bcab9e9a", APIVersion:"v1", ResourceVersion:"370", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_a3ac7a74-0e64-41bb-a2c4-a46c44eb5f68 became leader
I1003 14:58:12.776794       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_a3ac7a74-0e64-41bb-a2c4-a46c44eb5f68!

